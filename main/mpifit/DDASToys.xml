<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.3//EN"
                      "file:///usr/share/xml/docbook/schema/dtd/4.5/docbookx.dtd
"
>
<book>
    <bookinfo>
      <title>DDAS Toys for E17011</title>
      <author><firstname>Ron</firstname><surname>Fox</surname></author>
      <revhistory>
          <revision>
             <revnumber>1.0</revnumber>
             <date>January 2020</date>
             <authorinitials>RF</authorinitials>
             <revremark>Original Release</revremark>
          </revision>
      </revhistory>
    </bookinfo>
<chapter>
    <title>Introduction</title>
    <para>
        The software for  E17011 introduces several tools.  These tools are,
        for the most part, intended to be run either on the Fishtank or at the
        ICER HPCC. 
    </para>
    <para>
        This document introduces and documents these tools.
    </para>
    <para>
        The end goal of these tools is to support parallelized analysis
        at a rate that allows the analysis of nearline data to at least keep
        up with and hopefully out-pace data acquisition.  The tools address
        pulse classification and pulse fitting.  Our thanks to Cagri Kaymak and
        Professor Metin Aktulga for their work creating the random forest
        classifier.
    </para>
    <para>
        Here is a simple list of the tools:
    </para>
    <itemizedlist>
        <listitem>
            <para>
                Pulsemaker2 - this creates training sets with single and
                double pulses for the classification system.
            </para>
        </listitem>
        <listitem>
            <para>
                The pulse classification system produced by Cagri Kaymak
                under the supervision of Prof. Metin Aktulga.  This system
                takes as input a set of traces and returns as output a
                probability the pulses are single or double pulses.
            </para>
        </listitem>
        <listitem>
            <para>
                EventEditor with the libFitEditor plugin - this software
                appends fit parameters for double and single pulses to the
                event fragments in an event.
            </para>
        </listitem>
        <listitem>
            <para>
                EventEditor with the libWrapper plugin - this software
                is similar to the previous package, however it first runs
                the traces in an event through the random forest classifier
                and tags event fragments with single/double pulse probabilities.
                Once that's done, the fitter can either fit all traces for
                singles and doubles or be guided by the classifier outputs
                to determine which fit to perform.
            </para>
        </listitem>
        <listitem>
            <para>
                tracepeek - Provides graphical display of the traces, fits and
                classifications of traces for events in unedited and edited
                event files.
            </para>
        </listitem>
    </itemizedlist>
    <para>
        Finally, many of these tools depend on NSCLDAQ libraries and programs.
        Since the ICER HPCC runs on a very different environment than the systems
        at the NSCL, a singularity container has been created that reproduces
        enough of that environment to have supported installation and
        use of the NSCLDAQ software at the HPCC.  Most of the packages
        described above must, therefore, be run under that container.
    </para>
</chapter>
<chapter>
    <title>
        Singularity containers and nscl-ubuntu-cuda.img
    </title>
    <para>
        Singularity containers make use of an overlay file system to provide
        the userland libraries and utilities of specific linux environments
        under the run-time environment of another system.  For example, The HPCC
        runs a version of CentOS, derived from Red Hat Enterprise Linux.
        Building NSCLDAQ software under CentOS is problematic because of
        packaging differences and package support problems.
    </para>
    <para>
        A Debian-like singularity image allows NSCLDAQ to be built under a
        compilation and library environment that readily supports this.
    </para>
    <para>
        A singularity image is a single (large) file that contains a file-system
        image for the user utilities of a specific distribution of linux.  The
        <command>singularity</command> command can be used to run commands,
        or event activate shells in the environment of that file system image.
        Finally, the <command>singularity</command> command allows chunks of the
        host filesystem to be mounted at specific points in the image container.
        This allows us to build minimal containers, use them to build the
        software we need and the mount that software where we expect to see
        it in NSCLDAQ systems.
    </para>
    <para>
        As we experimented with using CUDA accelerated fitting, we started with
        an ubuntu bsaed image that was published by NVIDIA. We added several
        chunks of software to that image:
    </para>
    <itemizedlist>
        <listitem>
            <para>
                Software pre-requisite for building OpenMPI from source.  This
                is because the version of OpenMPI distributed for this version
                of Ubuntu is buggy when used with singularity.
            </para>
        </listitem>
        <listitem>
            <para>
                Software pre-requisite to building NSCLDAQ, SpecTcl and CERN ROOT
                version 6.13-08.  This is the last versio nof ROOT under which
                SpecTcl 5.2 can be built (specifically the last version that
                includes libQtGSI).
            </para>
        </listitem>
    </itemizedlist>
    <para>
        We then used the container to build the following software in a directory
        tree built to be mounted in the container at /usr/opt:
    </para>
    <itemizedlist>
        <listitem>
            <para>
                Root version 6.13-08 (in /usr/opt/root)
            </para>
        </listitem>
        <listitem>
            <para>
                OpenMPI-3.1.4 (in /usr/opt/openmpi-3.1.4).  This version of
                OpenMPI works reasonably well with singularity containers.
                Specifically, it can reliably support MPI jobs that have approximately
                500 processes.  More on how to run MPI jobs in containers in a bit.
            </para>
        </listitem>
        <listitem>
            <para>
                nscldaq version 11.4-002.  This was coupled with DDAS version
                3.1-002. (In /usr/opt/daq/11.4-002 and /usr/opt/ddas/3.1-002
                respectively).
            </para>
        </listitem>
        <listitem>
            <para>
                SpecTcl versions 5.2-001 and 5.2-003 (in
                /usr/opt/spectcl/{5.2-001,5.2-003}).
            </para>
        </listitem>
        <listitem>
            <para>
                The ddastoys directory tree in /usr/opt/ddastoys.
            </para>
        </listitem>
        <listitem>
            <para>
                Pulse classificataion and support software in
                ~/pulseclassification (the home directory is mounted by
                default in singularity containers).
            </para>
        </listitem>
    </itemizedlist>
    <section>
        <title>OpenMPI and Singularity Containers</title>
        <para>
            OpenMPI and Singularity containers play well togehter...mostly.
            To run MPI jobs requires OpenMPI in both the host and the
            container that have the same version (the documentation says
            compatible vesions but the same version works much better).
        </para>
        <para>
            Let's have a look at the following ICER HPCC SLURM script:
        </para>
        <informalexample>
            <programlisting>
#!/bin/bash

##
#  This file submits the fitter run inside the container.

# --ntasks are workers + 3
#  one hour run time is long enough for a 2GB segment I think



module purge

module load GCC/8.3.0           <co id='modules' />
module load OpenMPI/3.1.4



#  Note the script run.bash is parameterized by the number of workers
#  to run.

nworkers=`expr $SLURM_NTASKS - 3`   <co id='workers' />


mpirun   -np $SLURM_NTASKS  singularity exec --nv \
    --bind $SCRATCH:/scratch,$HOME/usropt:/usr/opt,/etc/libibverbs.d \
    $HOME/nscl-ubuntu-cuda.img $HOME/srun.bash $nworkers   <co id='mpirun'/>






            </programlisting>
        </informalexample>
        <para>
            We assume that all SLURM job control parameters (--ntasks,
            --time etc.), are provided on the sbatch command line.
        </para>
        <calloutlist>
            <callout arearefs='modules'>
                <para>
                    This set of lines unloads all modules and loads the
                    prerequisites for and the module for OpenMPI-3.1.4.
                    This module supports running jobs under MPI.
                </para>
            </callout>
            <callout arearefs='workers'>
                <para>
                    The environment variable <literal>SLURM_NTASKS</literal> is
                    set by slurm to contain the <option>--ntasks</option>
                    option value. 
                </para>
            </callout>
            <callout arearefs='mpirun'>
                <para>
                    This uses mpirun to start the container.
                    The command <command>singularity exec</command> runs
                    a command inside of the singularity container.
                </para>
                <para>
                    The <option>--bind</option> option binds bits of the
                    hosts filesystem to mount-points in the container. Its value
                    consists of a set of colon separated mount specifications.
                    Each mount specification consists of a local filesystem
                    path and optionally, followed by a <literal>,</literal>
                    a mount point in the container filesystem where that
                    path will appear.
                </para>
                <para>
                    The command contains two mount specifications.  The first
                    says that <filename>~/usropt</filename> will be mounted
                    in the container at <filename>/usr/opt</filename>.  This
                    tree contains the software we built under the container
                    described previously.  The second mount point mounts
                    a description of the MPI transports available on the host
                    system where OpenMPI expects it to be.  Doing this allows
                    OpenMPI software in the container to make use of the
                    Infiniband interconnections available in the host systems.
                </para>
                <para>
                    The container image is next it is ~/nscl-ubuntu-cuda.img.
                    This is followed by the command to run, which is a script
                    to run the actual program passing it the number of
                    worker processes to use.
                </para>
            </callout>
        </calloutlist>
        <para>
            Let's look at the sample <filename>srun.bash</filename>:
        </para>
        <informalexample>
            <programlisting>
#!/bin/bash

cd $HOME

. /usr/opt/daq/11.4-002/daqsetup.bash       <co id='daqsetup' />




export FIT_CONFIGFILE=/scratch/fitconfig.txt    <co id='jobenvsettings' />
export PATH=/usr/opt/openmpi-3.1.4/bin:$PATH     <co id='mpienvsettings' />
export LD_LIBRARY_PATH=/usr/opt/openmpi-3.1.4/lib

. /usr/opt/root/bin/thisroot.sh                 <co id='rootsetup' />

clump=500

echo job $SLURM_JOB_ID starting with $1 workers scratch to scratch clump: $clump

time $DAQBIN/EventEditor -l /usr/opt/ddastoys/lib/libFitEditor.so \  <co id='eventedit' />
                    -s file:///scratch/run-0011-00.evt \
                    -S file:///scratch/test-$SLURM_JOB_ID.out \
                    --workers=$1 --clump-size=$clump \
                    --parallel-strategy=mpi &gt;fit-$SLURM_JOB_ID.log 2&gt;&amp;1 &amp;

            </programlisting>
        </informalexample>
        <calloutlist>
            <callout arearefs='daqsetup'>
                <para>
                    Since the program we are running depends on NSCLDAQ,
                    we set up the NSCLDAQ environment variables.  Note that
                    since this script is running inside the container,
                    the <option>--bind</option> options we used put
                    the NSCLDAQ software in
                    <filename>/usr/opt/daq/11.4-002</filename> in the container.
                </para>
            </callout>
            <callout arearefs='jobenvsettings'>
                <para>
                    This environment variable is needed by the application.
                    We'll say more about it when we describe the
                    <filename>libFitEditor.so</filename> plugin.
                </para>
            </callout>
            <callout arearefs='mpienvsettings'>
                <para>
                    These environment settings point to the MPI software
                    built in the container.  Note again that
                    <option>--bind</option> values put that inside the
                    <filename>/usr/opt/openmpi-3.1.4</filename>
                    directory tree in the container.
                </para>
            </callout>
            <callout arearefs='rootsetup'>
                <para>
                    The software we're running also depends on Root. This
                    line sets up the Root environment variables.  Again,
                    thanks to <option>--bind</option>, Root is in
                    <filename>/usr/opt/root</filename> from the point of view
                    of the container.
                </para>
            </callout>
            <callout arearefs='eventedit'>
                <para>
                    Runs the MPI proram.  We'll say more about this
                    specific program later in this manual.
                </para>
            </callout>
        </calloutlist>
        <para>
            To summarize:
        </para>
        <itemizedlist>
            <listitem>
                <para>
                    We have a SLURM job that sets up the specific version of
                    OpenMPI we depend on and uses <command>mpirun</command>
                    to run a script in the container environment in parallel.
                </para>
            </listitem>
            <listitem>
                <para>
                    The script uses software in the container and set up
                    appropriate envornment variables needed to run the
                    actual program, then runs it.
                </para>
            </listitem>
        </itemizedlist>
    </section>
</chapter>
<chapter>
    <title>Pulse classification.</title>
    <para>
        This chapter describes the pulse classification software and the
        software available to make systematic training sets.  We'll start
        with the training set software (Pulsemaker2), include Cagri's training
        instructions for the classifier, which generate a model used by the
        classifier.  Finally we'll describe the classified fitting software.
    </para>
    <section>
        <title>Pulsemaker2</title>
        <para>
            Supervised learning systems are only as good a the training data.
            Classifiers can be thought of as systems that fit an unknown function
            to a set of points (the training set).  In order to extrapolate the
            function realistically to points that are not in the training data,
            it is important to have data that covers the entire parameter
            space of interest.
        </para>
        <para>
            Pulsemaker creates training data by randomly generating points
            over the parameter space.  For each pulse, each parameter is drawn
            from a uniform random distribution over an allowed range of parameter
            values.  Given a sufficient number of pulses, this can result in
            reasonable coverage of the parameter space.
        </para>
        <para>
            Pulsemaker2, creates training data systematically.  It divides the
            parameter space into a grid whose resolution you can set and
            scans that space, generating a pulse (or double-pulse) at each
            point on that grid.  This can be time consuming, so a parallelized
            version of Pulsemaker2 has also been written.  mpiPulseMaker2 hands out
            subsets of the parameter hypervolume to parallel workers to
            improve the time performance of the parameter space scan.
        </para>
        <para>
            First let's look at the command line parameters to Pulsemaker2.
            Second we'll look at SLURM scripts that can run mpiPulseMaker2.
            Note that since mpiPulseMaker2 has no NSCLDAQ dependencies it can
            be run natively on ICER's HPCC or the NSCL Fireside cluster.
        </para>
        <para>
            There are two actual forms of the Pulsemaker2 command line.  Which you
            use depends on whether you are generating single or double pulses.
        </para>
        <informalexample>
            <cmdsynopsis>
                <command>
pulseMaker2 file-name 1 position rise fall scale offset tracelen sat
                </command>    
            </cmdsynopsis>
            <cmdsynopsis>
                <command>
pulsemaker2 file-name 2 pos1 rise1 fall1 \
             scale1 pos2 rise2 fall2 scale2 offset tracelen sat
                </command>
            </cmdsynopsis>
        </informalexample>
        <para>
            Before describing the command parameters I want to describe how an
            axis of the parameter space is described.  Parameter space axes are
            three comma separated elements. The first element is the low limit,
            the second element is the high limit and the last the step size.
            There will always be at least one point at the low value (if the step
            size is larger than high-low, there will be only that point).
        </para>
        <para>
            The firs two command parameters for pulseMaker2 are the output
            filename, and the number of pulses created per output trace.  
        </para>
        <para>
            If the number of pulses is <literal>1</literal> the following
            parameters will be present:
        </para>
        <variablelist>
            <varlistentry>
                <term><parameter>position</parameter></term>
                <listitem>
                    <para>
                        This is an axis specification for the pulse position
                        (that is the x0 of the pulse function).  By axis
                        specification I do mean a three element comma separated
                        list (e.g. 500,600,1 means from 500 to 600 with a stepsize of
                        1).
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term><parameter>rise</parameter></term>
                <listitem>
                    <para>
                        The rise speed axis specification.  This is the
                        steepness parameter of the logistic part of the
                        pulse function.
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term><parameter>fall</parameter></term>
                <listitem>
                    <para>
                        The fall speed axis specification.  This is the
                        decay constant of the decay part of the pulse function.
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term><parameter>scale</parameter></term>
                <listitem>
                    <para>
                        The scale factor axis specification.  This is the
                        multiplier of the pulse function.
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term><parameter>offset</parameter></term>
                <listitem>
                    <para>
                        The DC offset axis specification.
                    </para>
                </listitem>
            </varlistentry><varlistentry>
                <term><parameter>tracelen</parameter></term>
                <listitem>
                    <para>
                        The length of the traces generated.
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term><parameter>sat</parameter></term>
                <listitem>
                    <para>
                        The value at which the pulse saturates.  If a generated
                        trace point would have a value larger than this,
                        the trace point value is set to this value.
                    </para>
                </listitem>
            </varlistentry>
            
        </variablelist>
        <para>
            If you are creating double pulses the remaining parameters are:
        </para>
        <variablelist>
            <varlistentry>
                <term><parameter>pos1</parameter></term>
                <listitem>
                    <para>
                        The axis specification of pulse 1.
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term><parameter>rise1 </parameter></term>
                <listitem>
                    <para>
                        The rise steepness axis specification of pulse 1.
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term><parameter>fall1</parameter></term>
                <listitem>
                    <para>
                        The decay constant axis specification of pulse 1.
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term><parameter>scale1 </parameter></term>
                <listitem>
                    <para>
                        The scale factor axis specification of pulse 1.
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term><parameter>pos2 </parameter></term>
                <listitem>
                    <para>
                        The position axis specification of pulse 2. Normally this
                        will have a low value no smaller than the low value for
                        pulse 1 as the second pulse will be later than pulse1
                        in time.  Pulsemaker does not care, however, if that is
                        not the case.
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term><parameter>rise2 </parameter></term>
                <listitem>
                    <para>
                        The pulse steepness axis specification for pulse 2.
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term><parameter>fall2 </parameter></term>
                <listitem>
                    <para>
                        The pulse decay axis specification for pulse 2.
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term><parameter>scale2 </parameter></term>
                <listitem>
                    <para>
                        The scale factor axis specification for pulse 2.
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term><parameter>offset </parameter></term>
                <listitem>
                    <para>
                        The DC offset axis specification.
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term><parameter>tracelen </parameter></term>
                <listitem>
                    <para>
                        The length of the trace (in samples).
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term><parameter>sat</parameter></term>
                <listitem>
                    <para>
                        The trace saturation value.
                    </para>
                </listitem>
            </varlistentry>
        </variablelist>
        <para>
            The program mpiPulseMaker2 is an MPI enabled parallel pulsemaker.
            It must be run with at least three processes.  One process, rank 0,
            distributes volumes of parameter space to workers.  A second process,
            rank 1 harvests the traces that were created by workers and write
            them to the output file.  All remaining ranks are workers.
            This allows the parameter hypervolume to be scanned with
            (n-2) parallelism.
        </para>
        <para>
            Here is a SLURM job that runs the mpi pulsemaker2 to generate
            single pulses:
        </para>
        <informalexample>
            <programlisting>
#!/bin/bash
#
#  Run pulsemaker for single pulse waveforms using MPI
#  to chunk the parameter space over severl workers.
#
#  sbatch --ntasks=number-workers+2 --time=time-limit pulsemaker1.bash outputfile
#
# You will need to adjust the pulse parameters in the mpirun command below.


module purge

module load GCC/8.3.0
module load OpenMPI/3.1.4



#  Note the script run.bash is parameterized by the number of workers
#  to run.

file=$1
echo Writing output to $file

cd ~/pulseclassification/pulsemaker2

mpirun   -np $SLURM_NTASKS ./mpiPulseMaker2 $file 1 \
    100,150,10 0.01,0.02,10 0.01,0.02,10 100,1000,900 0,100,10 \
    250 8192


            </programlisting>
        </informalexample>
        <para>
            Similarly, here's a SLURM batch script for double pulses:
        </para>
        <informalexample>
            <programlisting>
#!/bin/bash
#
#  Run pulsemaker for double  pulse waveforms using MPI
#  to chunk the parameter space over several workers.
#
#  sbatch --ntasks=number-workers+2 --time=time-limit pulsemaker2.bash outputfile
#
# You will need to adjust the pulse parameter in the mpirun command below.


module purge

module load GCC/8.3.0
module load OpenMPI/3.1.4



#  Note the script run.bash is parameterized by the number of workers
#  to run.

file=$1
echo Writing output to $file

cd ~/pulseclassification/pulsemaker2

mpirun   -np $SLURM_NTASKS ./mpiPulseMaker2 $file 2 \
    100,150,5 0.01,0.02,2 0.01,0.02,2 100,1000,5 \
    101,250,5 0.01,0.02,2 0.01,0.02,2 100,1000,5 \
    0,100,5 \
    250 8192


            </programlisting>
        </informalexample>
        <para>
            Note that for E17011, we plan to install the software centrally so
            the cd will need to be adjusted as well as the parameter axis
            specifications.  Note as well that you don't need to run this in
            the container.
        </para>
        <para>
            The output file generated will be binary each trace will consist of
            a 16 bit integer trace length (in samples) followed by that number
            of 16 bit integer sample values.  This is the form expected by
            the first phase of the classifier training procedure.
        </para>
    </section>
    <section>
        <title>Training the classifier.</title>
        <para>
            This section yet to be written.
        </para>
    </section>
</chapter>
<chapter>
    <title>Event Editing programs</title>
    <para>
        The <filename>nscl-ubuntu-cuda.img</filename> container supports
        NSCLDAQ 11.4 which includes parallel event editors.  The
        <command>EventEditor</command> program provides a plug in architecture
        for editing events.   We use this program with two plugins:
    </para>
    <itemizedlist>
        <listitem>
            <para>
                Fit the traces in hits appending the fit information
                to each hit.
            </para>
        </listitem>
        <listitem>
            <para>
                Run the classifier appending its results to the
                body headers of each hit and optionally performing fits
                steered by the results of the classifier.
            </para>
        </listitem>
    </itemizedlist>
    
    <section>
        <title>Event fitting</title>

        <para>
            Event fitting can be done in MPI parallel inside the container at
            ICER's HPCC.  Inside the container the /usr/opt/ddastoys/lib
            directory contains a <filename>libFitEditor.so</filename> plugin
            for <filename>$DAQBIN/EventEditor</filename>.
        </para>
        <para>
            Here are sample scripts that run this at ICER:
        </para>
        <informalexample>
            <programlisting>
#!/bin/bash

##
#  This file submits the fitter run inside the container.

# --ntasks are workers + 3
#  one hour run time is long enough for a 2GB segment I think



module purge

module load GCC/8.3.0
module load OpenMPI/3.1.4



#  Note the script run.bash is parameterized by the number of workers
#  to run.

nworkers=`expr $SLURM_NTASKS - 3`


mpirun   -np $SLURM_NTASKS  singularity exec --nv \
    --bind $SCRATCH:/scratch,$HOME/usropt:/usr/opt,/etc/libibverbs.d \
    $HOME/nscl-ubuntu-cuda.img $HOME/srun.bash $nworkers

wait
            </programlisting>
        </informalexample>
        <para>
            As indicated in the comments. sbatch must specify an
            <option>--ntasks</option>  value of at least 4.  This is because
            in addition to fit tasks there are tasks that distribute the data,
            sort the fitted data and output the sorted data to file.
        </para>
        <para>
            Note that mpirun in the script starts the container to run
            a script.  Here's what that script should look like:
        </para>
        <informalexample>
            <programlisting>
#!/bin/bash

cd $HOME

. /usr/opt/daq/11.4-002/daqsetup.bash




export FIT_CONFIGFILE=/scratch/fitconfig.txt
export PATH=/usr/opt/openmpi-3.1.4/bin:$PATH
export LD_LIBRARY_PATH=/usr/opt/openmpi-3.1.4/lib

. /usr/opt/root/bin/thisroot.sh

clump=500

echo job $SLURM_JOB_ID starting with $1 workers scratch to scratch clump: $clump

time $DAQBIN/EventEditor -l /usr/opt/ddastoys/lib/libFitEditor.so \
                    -s file:///scratch/run-0011-00.evt \
                    -S file:///scratch/test-$SLURM_JOB_ID.out \
                    --workers=$1 --clump-size=$clump --parallel-strategy=mpi \
                    &gt;fit-$SLURM_JOB_ID.log &gt;>&amp;1 &amp;

            </programlisting>
            <para>
                The <option>-s</option> option specifies the input URI and
                the <option>-S</option> option specifies the output URI.  You
                could make these parameters of the slurm batch job that get passed
                in and used on the command line of the script above.
            </para>
            <para>
                <varname>clump</varname> in the script above determines how many
                events get passsed as a work unit to each worker.  Larger
                numbers can reduce the communication overhead.  
            </para>
        </informalexample>
        <para>
            The plugin adds information that describes the single and double
            pulse fits it performs for each trace.  The sizes for the whole
            event and the sizes for the fragment are updated to be consistent
            with the updated event sizes.
        </para>
        <para>
            The extension added to each trace is described in
            <filename>/usr/opt/ddastoys/include/lmfit.h</filename>  See
            the struct <structname>HitExtension</structname> in the
            <literal>DDAS</literal> namespace.
        </para>
        <para>
            The environment variable <literal>FIT_CONFIGFILE</literal> points to
            a text file that describes what to fit and how.  This file is a text
            file.  Each line can be empty, start with a <literal>#</literal> to
            indicate it is a comment or be a fit descriptor.
            The fit descriptor lines have  6 whitespace separated fields. These
            fields are all integer and are in order the digitizer crate, slot, channel
            start sample, end sample, and saturation value.
        </para>
        <para>
            Digitizer channels that don't occur in this file are not fit. Specifying
            the start and end samples allows you to exclude regions that may
            have transient spikes at the start and end of the trace. Values
            that are at or above the saturation value are excluded from the fit
            as they are assumed to be flat-tops due to pulses that saturate
            the ADC.
        </para>
    </section>
    <section>
        <title>Classifier steered fitting.</title>
        <para>
            The <filename>libWrapper.so</filename> plugin to the
            event editor collects the traces from the event fragments in an
            event and feeds them to the classifier. It then:
        </para>
        <itemizedlist>
            <listitem>
                <para>
                    Appends a 32 bit word to the fragment's body header.  This word
                    contains scaled probabilities produced by the classifier.
                    The low 16 bits contains the classifier's probability
                    the trace contains two pulses while the upper 16 bits
                    contains the probability the trace contains only one
                    pulse.  The probabilities are fixed point values.
                    Each floating point probability is multiplied by 10,000 and
                    integerized.
                </para>
            </listitem>
            <listitem>
                <para>
                    Appends the same <structname>HitExtension</structname> produced
                    by <filename>libFitEditor.so</filename> described above.
                    Optionally only the fit indicated by the classifier is
                    performed.  In this case, all values for the fit not
                    performed are set to 0.
                </para>
            </listitem>
        </itemizedlist>
        <para>
            Let's have a look at the scripst needed to run this under MPI.
            Note that while <filename>EventEditor</filename> can be run
            with a threaded parallelization strategy, this is not supported
            with <filename>libWrapper.so</filename> for worker counts larger
            than 1 because the classifier is not thread-safe.
        </para>
        <informalexample>
            <programlisting>
#!/bin/bash

##
#  This file submits the fitter run inside the container.

# --ntasks are workers + 3
#  one hour run time is long enough for a 2GB segment I think


module purge

module load GCC/8.3.0
module load OpenMPI/3.1.4



#  Note the script run.bash is parameterized by the number of workers
#  to run.

nworkers=`expr $SLURM_NTASKS - 3`


mpirun -np $SLURM_NTASKS singularity exec \
    --bind $SCRATCH:/scratch,$HOME/usropt:/usr/opt,/etc/libibverbs.d \
    $HOME/nscl-ubuntu-cuda.img $HOME/pulse.bash $nworkers

wait

            </programlisting>
        </informalexample>
        <para>
            There should be nothing much new here.  
        </para>
        <para>
            Here is the <filename>pulse.bash script</filename>:
        </para>
        <informalexample>
            <programlisting>
#!/bin/bash
echo "Running pulse.bash $1 workers"
cd $HOME/pulseclassification

echo directory set.

export FIT_CONFIGFILE=/scratch/fitconfig.txt
export CLASSIFIER_MODEL=$HOME/pulseclassification/model_num_comp_25_n_est_200_depth_16_min_leaf_8_min_split_8.pkl
export CLASSIFIER_TRACELEN=250
export DOUBLE_PULSE_THRESHOLD=0.5

export PATH=/usr/opt/openmpi-3.1.4/bin:$PATH
export LD_LIBRARY_PATH=/usr/opt/openmpi-3.1.4/lib
export PYTHONPATH=$HOME/pulseclassification #   Where the classify module is.

echo exports done


. /usr/opt/root/bin/thisroot.sh
echo root setup
. /usr/opt/daq/11.4-002/daqsetup.bash
echo nscldaq setup.



time $DAQBIN/FullEventEditor \
     -s file:///scratch/run11_4.evt -S file:///scratch/test-$SLURM_JOB_ID.evt \
     -l $HOME/pulseclassification/ClassifiedEditor.so \
     --workers=$1 --clump-size=100 --parallel-strategy=mpi \
    &gt;$HOME/class-$SLURM_JOB_ID.log 2&gt;&amp;1 &amp;



            </programlisting>
        </informalexample>
        <para>
            A bit about the environment variables in the last script:
        </para>
        <variablelist>
            <varlistentry>
                <term><literal>FIT_CONFIGFILE</literal></term>
                <listitem>
                    <para>
                        Points to the fit configuration file as in the
                        previous section.  Digitizer channels that don't appear
                        in this file are not processed/edited by this plugin.
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term><literal>CLASSIFIER_MODEL</literal></term>
                <listitem>
                    <para>
                        Points to the classifier model file.  This file is produced
                        by training the model on data, usually produced by
                        pulesemaker2's serial or MPI parallel version.
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term><literal>CLASSIFIER_TRACELEN</literal></term>
                <listitem>
                    <para>
                        Provides the length of the traces used to train the classifier.
                        It can happen that the actual trace lengths are different
                        from this. If actual traces are longer than this value,
                        they are truncated on the right prior to being handed
                        to the classifier. If actual traces are shorter than this
                        value, they are extended with an estimated background
                        value before being handed to the classifier.
                    </para>
                    <para>
                        Note that this value does not affect the traces handed to
                        the fitter.
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term><literal>DOUBLE_PULSE_THRESHOLD</literal></term>
                <listitem>
                    <para>
                        This variable is optional.  If provided and non-zero,
                        the value must be a floating point value in the range
                        (0,1]. If a trace is classified as having a double
                        pulse probability higher than DOUBLE_PULSE_THRESHOLD,
                        only the double pulse fit is performed.  If not, only
                        the single pulse fit is performed.  The fit data
                        for the fit not performed are all set to 0.
                    </para>
                    <para>
                        If this environment variable is not defined or is 0,
                        both single and double pulse fits are performed.
                    </para>
                </listitem>
            </varlistentry>
        </variablelist>
    </section>
</chapter>
<chapter>
    <title>tracepeek</title>
    <para>
        Tracepeek is a ddastoys program that allows you to view DDAS event data
        with traces.  If it detects an extension to the body header of a hit, the
        classifier probabilities are displayed.  If it detects a fit extension,
        the fit(s) are displayed as well.
    </para>
    <para>
        Tracepeek must be run under the container.  
    </para>
    <orderedlist>
        <listitem>
            <para>
                Start the container:
            </para>
            <informalexample>
                <programlisting>
singularity shell --bind $SCRATCH,/scratch,$HOME/usropt,/usr/opt:/etc/libibverbs.d \
    nscl-ubuntu-cuda.img
                </programlisting>
            </informalexample>
        </listitem>
        <listitem>
            <para>
                Setup  NSCLDAQ and root:
            </para>
            <informalexample>
                <programlisting>

. /usr/opt/daq/11.4-002/daqsetup.bash
. /usr/opt/root/bin/thisroot

                </programlisting>
            </informalexample>
            
        </listitem>
        <listitem>
            <para>
                Start the program with the appropriate directories added to the
                Tcl library search path (the program is a Tcl script with external
                package dependencies).
            </para>
            <informalexample>
                <programlisting>
TCLLIBPATH=" /usr/opt/ddastoys/lib $DAQROOT/TclLibs"  /usr/opt/ddastoys/bin/tracepeek
                </programlisting>
            </informalexample>
        </listitem>
    </orderedlist>
    <para>
        You'll then get a file chooser dialog you can use to select the event
        file you want to peek into.  The only thing that might not be pretty
        much intuitive to the operation of the program is that if you drag a
        region of interest on the plot window and release the plot will
        expand in X and Y to display essentially only that region of interest.
        To reset the expansion  simply reselect the channel you're viewing.
    </para>
    <para>
        The labels above the plot provide fit information if available and
        as well as classification probabilities, if available.  If fits were
        performed, they are superimposed on the traces in the plot window.
    </para>
</chapter>
<appendix>
        <title>Python pre-requisites for using the classifier.</title>
        <para>
            To satisfy the Python version dependencies and package dependencies
            you must:
        </para>
        <itemizedlist>
            <listitem>
                <para>
                    Always run it from within the container.
                </para>
            </listitem>
            <listitem>
                <para>
                    Perform local installs of the prerequisite python packages
                    prior to the first use of the classifier.  This installation
                    should be done within the container to ensure the correct
                    software versions are installed:
                </para>
                <informalexample>
                    <programlisting>
singularity shell --bind $SCRATCH,/scratch,$HOME/usropt,/usr/opt:/etc/libibverbs.d \
    nscl-ubuntu-cuda.img
python3 -mpip install --user sklearn
python3 -mpip install --user joblib
exit
                    </programlisting>
                </informalexample>
            </listitem>
        </itemizedlist>
        <para>
            Be sure to specify <command>python3</command> in the commands
            above as the default version of Python in the container is
            Python 2.7.15+ while the ML software used by the classifier
            requires Python version 3.  
        </para>
        <para>
            These commands use the container to install the sklearn
            machine learning software and the joblib job control library
            on a per user basis, along with their pre-requisites.
            Note that these installations will
            put their packages in <filename>~/.local/lib/python3.6</filename>.
            This directory tree is part of the Python default package search
            path.
        </para>
        <para>
            It is important to use the container so that the packages installed
            are compatible with the exact version of the Python interpreter
            in the container rather than in the host system.
        </para>

</appendix>
</book>

