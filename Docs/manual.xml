<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.3//EN"
"/usr/share/xml/docbook/schema/dtd/4.5/docbookx.dtd">

<book>
  
  <bookinfo>
    <title>DDASToys Manual</title>
    <authorgroup>
      <author>
	<firstname>Ron</firstname><surname>Fox</surname>
	<affiliation>
	  <orgname>
	    Facility for Rare Isotope Beams, Michigan State University, East Lansing, MI 48824
	  </orgname>
	</affiliation>
      </author>
      <author>
	<firstname>Aaron</firstname><surname>Chester</surname>
	<affiliation>
	  <orgname>
	    Facility for Rare Isotope Beams, Michigan State University, East Lansing, MI 48824
	  </orgname>
	</affiliation>
      </author>
    </authorgroup>
    <revhistory>
      <revision>
        <revnumber>6.0-000</revnumber>
        <date>9/24/2024</date>
        <authorinitials>RF, ASC</authorinitials>
        <revremark>Original release</revremark>
      </revision>
    </revhistory>
  </bookinfo>
  
  <chapter id="ch_intro">
    <title>Introduction</title>    
    <para>
      The DDASToys software suite supports parallelized trace fitting at a rate that allows the analysis of nearline data to at least keep up with&#x2014;and hopefully outpace&#x2014;data acquisition. Several tools are introduced as part of this software package. These tools are, for the most part, intended to be run on a remote or local computing cluster (e.g., NERSC's Perlmutter or the FRIB nucleus system).
    </para>
    <para>
      This documentation is valid for DDASToys versions 4.0-000 and later. DDASToys major versions 4 and 5 are compatible with FRIBDAQ 12.0-005 and later; version 6 of the DDASToys codebase requires FRIBDAQ 12.1-000. Bug reports and other issues related to the DDASToys software should be addressed to daqhelp@frib.msu.edu.
    </para>
    <para>
      This document introduces and describes the tools provided by the DDASToys package. A worked example for the Perlmutter system at NERSC demonstrates how to access and configure your user environment to use these tools and how to submit jobs at using the Slurm Workload Manager. Other computing clusters may use a different resource management system (e.g, PBS at ALCF Polaris) but the concepts of resource provisioning and managing results are generally applicable. In all cases, documentation provided by the facility the code is being run at should be consulted alongside this guide. All ambiguities or conflicts should be resolved by following the advice of the facility documentation. Though much of this document is oriented towards running DDASToys codes at NERSC, the use of Slurm at many High Performance Computing (HPC) facilities allows the example scripts to be customized for applications outside of NERSC with minimal reconfiguration.
    </para>
    <para>
      Detailed documentation of the DDASToys source code are provided in Doxygen reference manuals, installed in the share/sourcedocs/html and share/sourcedocs/latex directories located under the top-level DDASToys installation directory, which is generally /usr/opt/ddastoys/&lt;version&gt;. To view the Doxygen documentation, open share/sourcedocs/html/index.html with a web browser such as Firefox.
    </para>
    <para>
      The DDASToys Manual is <emphasis>not</emphasis> intended as a comprehensive guide to NERSC systems or as a troubleshooting document to address issues you may encounter while running your code at NERSC or any other computing facility. Please refer to NERSC's own documentation pages (or the documentation pages for the appropriate facility) for more information: <ulink url="https://docs.nersc.gov/"/>.
    </para>
    <para>
      The tools provided by this package are:
    </para>
    <itemizedlist>
      <listitem>
	<para>
	  FRIBDAQ <literal>EventEditor</literal> <literal>FitEditor</literal> library plugins - Parallel trace-fitting software which appends fit parameters for single- and double-pulse fits to fragments (hits) in an event, modifying the event structure.
	</para>
      </listitem>
      <listitem>
	<para>
	  <literal>libFitEditorAnalytic.so</literal> plugin library - A fitting library which models the pulse shape using a constant baseline offset, a logistic risetime and an exponential decay. The fitting routines use the Levenburg-Marquardt (LM) algorithm implemented in the Gnu Scientific Library (GSL). The fit parameters are determined by chi-square minimization.
	</para>
      </listitem>
      <listitem>
	<para>
	  <literal>libFitEditorTemplate.so</literal> plugin library - GSL LM fitting library which  models the detector response using an idealized template pulse shape. The trace template is provided by the user and is typically constructed from experimental data.
	</para>
      </listitem>
      <listitem>
	<para>
	  <literal>libFitEditorMLInference.so</literal> plugin library - Machine-learning inference fitting for trace data. The detector response is modeled using the analytic fitting functions and ML inference is used to extract parameters for a given trace from a trained model. The ML model provides some means to classify event as either single- or double-pulse hits. First available as part of DDASToys release 6.0-000.
	</para>
      </listitem>
      <listitem>
	<para>
	  <literal>libDDASFitHitUnpacker.so</literal> library - A library for unpacking DDAS hit data which may contain fit extensions added to the event by the <literal>EventEditor</literal>. Also capable of unpacking hits without fit extensions and old-style fit extensions coming from legacy programs like the <literal>ringblockdealer</literal>.
	</para>
      </listitem>
      <listitem>
	<para>
	  <literal>traceview</literal> - A program to interactively display traces, their associated fits (if present), and, if defined, machine learning pulse classification probabilities for events in edited and unedited event files.
	</para>
      </listitem>
      <listitem>
	<para>
	  <literal>eeconverter</literal> - A program to convert FRIBDAQ event files with (or without) fit extensions to ROOT format. Each fragment in an event is converted into a DDASFitHit object and each event is written as an entry in an output ROOT TTree saved in a ROOT TFile.
	</para>
      </listitem>
      <listitem>
	<para>
	  <literal>libDDASRootFitFormat.so</literal> - A library containing the definition and implementation of the custom classes streamed to the output ROOT file. User code which loads <literal>eeconverter</literal> output files should link against this library.
	</para>
      </listitem>
    </itemizedlist>      
    <para>
      Finally, all of these tools depend on FRIBDAQ libraries and programs. A Shifter image has been created that reproduces enough of that environment to support installation and use of the FRIBDAQ software at NERSC. Most of the packages described above must, therefore, be run under a container running the image.
    </para>    
  </chapter>

  <chapter id="ch_shifter">
    <title>
      Shifter Images and Containerized FRIBDAQ
    </title>
    <para>
      In this chapter we will discuss how to run FRIBDAQ software under a container, using shifter as an example. Shifter is a tool for adapting concepts from Linux containers to HPC resources, allowing users to create their own software environment and run it at a supercomputing facility like NERSC. Conceptually this is very similar to the singularity or apptainer containers FRIB users are familiar with. Generally speaking, containers make use of an overlay file system to provide the userland libraries and utilities of specific Linux environments under the run-time environment of another system. A Debian-like image allows FRIBDAQ to be built under a supported compilation and library environment.
    </para>    
    <para>
      A Shifter image is a single (large) file that contains a file-system image for the user utilities of a specific distribution of Linux. The <command>shifter</command> command can be used to run other commands, set environment variables, or even activate shells in the environment of a container running an instance of a file-system image. Finally, the <command>shifter</command> command allows pieces of the host file system to be mounted at specific points in the container. This allows us to build minimal containers, use them to build the software we need and the mount that software where we expect to see it in FRIBDAQ systems, for example, the /usr/opt directory tree familiar to FRIB users.
    </para>    
    <para>
      The Shifter image available on NERSC, fribdaq/frib-buster:v4.2, is a Debian-10-based Buster image with several chunks of software added. Note that the exact contents of the image depend on the DDASToys, FRIBDAQ, and OS distribution you use. In the fribdaq/frib-buster:v4.2, we have:
    </para>
    <itemizedlist>
      <listitem>
	<para>
	  Software required for building OpenMPI from source.
	</para>
      </listitem>
      <listitem>
	<para>
	  Software required to build FRIBDAQ, including CERN ROOT version 6.24.06, XIA API version 2.13 and the Broadcom PLX driver version 7.25.
	</para>
      </listitem>
      <listitem>
	<para>
	  Software required to build SpecTcl.
	</para>
      </listitem>
    </itemizedlist>
    <para>
      We then use the image to build the following software in a directory tree to be mounted in the image at /usr/opt:
    </para>
    <itemizedlist>
      <listitem>
	<para>
	  OpenMPI 4.0.1 (in /usr/opt/mpi/openmpi-4.0.1). This version of OpenMPI works reasonably well in singularity containers based on our experience with ICER at MSU and works "out of the box" at NERSC. More on how to run MPI jobs in containers in a bit.
	</para>
      </listitem>
      <listitem>
	<para>
	  ROOT version 6.24.06 (in /usr/opt/root/root-6.24.06).
	</para>
      </listitem>
      <listitem>
	<para>
	  FRIBDAQ version 12.0-005, a production version of FRIBDAQ with refactored DDAS code included (in /usr/opt/daq/12.0-005).
	</para>
      </listitem>
      <listitem>
	<para>
	  SpecTcl version 5.13-008.1 (in /usr/opt/spectcl/5.13-008.1).
	</para>
      </listitem>
      <listitem>
	<para>
	  The DDASToys directory tree (in /usr/opt/ddastoys).
	</para>
      </listitem>	
    </itemizedlist>
    <para>
      These specific software versions are tied to the buster usropt tree, if you elect to use an image derived from the FRIB bullseye container the bullseye usropt tree will contain different versions of the above software installed in the same locations. To illustrate the use of Shifter to run containerized applications, consider the following command to spawn a bash shell inside our container:
      <informalexample>
	<programlisting>
	  shifter                          \ <co id="shifter"/> 
	  --image=fribdaq/frib-buster:v4.2 \ <co id="simage"/>
	  --module=none                    \ <co id="smodules"/>
	  --volume=$SCRATCH:/scratch       \ <co id="svolumes"/>
	  --env-file=$HOME/shifter.env     \ <co id="senvfile"/>
	  /bin/bash                          <co id="scmd"/>
	</programlisting>
      </informalexample>
      <calloutlist>
	<callout arearefs="shifter">
	  <para>
	    Run Shifter with the supplied flags.
	  </para>
	</callout>
	<callout arearefs="simage">
	  <para>
	    A mandatory flag to select the Shifter image. In this case, our fribdaq/frib-buster:v4.2 image.
	  </para>
	</callout>
	<callout arearefs="smodules">
	  <para>
	    An optional flag to specify which modules to load. No modules are loaded in order to avoid conflicts with our OpenMPI installation and the Cray MPICH module at NERSC.
	  </para>
	</callout>	
	<callout arearefs="svolumes">
	  <para>
	    An optional flag to specify volume mounting points inside the container. The syntax is <literal>--volume directory_outside_container:target_in_container</literal>, with multiple directories separated by semicolons. In this case, the user scratch space is mounted inside the container at /scratch. For users familiar with <literal>singularity</literal> or <literal>apptainer</literal>, this is similar to the <literal>--bind</literal> flag.
	  </para>
	</callout>
	<callout arearefs="senvfile">
	  <para>
	    An optional flag to configure environment variables inside the shifter image. All environment variables defined in the calling process' environment are transferred into the image. Additional environment variables are defined in the shifter.env file provided to this flag. The contents of this file will be discussed in <xref linkend="sec_configfiles"/> when we discuss how to run DDASToys applications at NERSC.
	  </para>
	</callout>
	<callout arearefs="scmd">
	  <para>
	    The command to run inside the Shifter container. In this case, we spawn an interactive Bash shell.
	  </para>
	</callout>
      </calloutlist>
      For a more detailed discussion of Shifter flags run the command <command>shifter -h</command> on Perlmutter or refer to the NERSC Shifter documentation available here: <ulink url="https://docs.nersc.gov/development/shifter/shifter-tutorial"/> and here: <ulink url="https://docs.nersc.gov/development/shifter/how-to-use"/>. A troubleshooting guide is available at <ulink url="https://docs.nersc.gov/development/shifter/faq-troubleshooting/"/>.
    </para>

    <section id="sec_bindpoints">
      <title>A Quick Note About Volume Mounting in Shifter</title>
      <para>
	Occasionally mounting a volume in the image using the <literal>--bind</literal> flag will fail and report either an <literal>invalid volume map</literal> or the following error:
	<informalexample>
	  <programlisting>
	    ERROR: unclean exit from bind-mount routine. /var/udiMount/&lt;path_in_image&gt;
	        may still be mounted.
	    BIND MOUNT FAILED from /var/udiMount/&lt;full_path_to_directory&gt;
	        to /var/udiMount/&lt;path_in_image&gt;
	    FAILED to setup user-requested mounts.
	    FAILED to setup image
	  </programlisting>
	</informalexample>
	 Most likely this is a permissions issue with the directory being mounted. Running the command <command>setfacl -m u:nobody:x &lt;full_path_to_directory&gt;</command> should resolve the issue. Only the owner of a directory is allowed to change its permissions. See <ulink url="https://docs.nersc.gov/development/shifter/faq-troubleshooting/#invalid-volume-map"/> for further information.
      </para>
    </section>
  </chapter>

  <chapter id="ch_gettingstarted">
    <title>Getting Started at NERSC</title>
    <section id="sec_createacct">
      <title>Creating a NERSC Account</title>
      <para>
	This guide assumes that the reader wishes to join an existing project. In order to join a project, you must create an account at NERSC. To do so, follow the "I need a new NERSC account" steps here: <ulink url="https://iris.nersc.gov/add-user"/>. Note that NERSC requires that you have an exciting ORCID ID. You can register for an ORCID account here: <ulink url="https://orcid.org/signin"/>. Once you have created an account at NERSC, you will receive two emails: the first informing you that the account request is being processed, and another when the account is approved and added to the project. This requires approval from both NERSC and the project PI, so the approval process may take a couple of days. For any account-related issues, contact accounts@nersc.gov.
      </para>
    </section>
    
    <section id="sec_login">
      <title>Logging in to NERSC Systems</title>
      <para>
	The machine we will use for analysis at NERSC is Perlmutter, a HPE Cray EX supercomputer. Logging in to Perlmutter requires multifactor identification (MFA). Instructions on how to set up MFA for your account at NERSC are found at <ulink url="https://docs.nersc.gov/connect/mfa"/>. You can optionally setup an SSH proxy to avoid entering your one time password (OTP) every time you log in by following the instructions provided at <ulink url="https://docs.nersc.gov/connect/mfa/#using-sshproxy"/>. Note that the generated key is only good for 24 hours. Once your MFA has been configured, you can connect directly to Perlmutter from the outside via SSH with the commands <command>ssh &lt;username&gt;@perlmutter-p1.nersc.gov</command> or <command>ssh &lt;username&gt;@saul-p1.nersc.gov</command> where &lt;username&gt; is replaced by your NERSC user account name. 
      </para>
    </section>
    
    <section id="sec_config">
      <title>Configuring Your Home Directory</title>
      <para>
	Connecting to NERSC systems using SSH will place you in your home directory on a login node. The first time you log in the home directory will be empty. This section describes some basic account configuration options which first-time users may find helpful.
      </para>
      <para>
	NERSC documentation is ambiguous with respect to the order in which shell initialization files ("dotfiles" such as ~/.bashrc) are sourced and under what conditions, but the user's ~/.bashrc appears to be sourced on both login and interactive non-login shells if it exists. NERSC provides a template .bashrc file available in the GitLab repository <ulink url="https://software.nersc.gov/NERSC/dotfiles"/>. Accessing the repository requires a NERSC account with MFA enabled.
      </para>
      <para>
	To clone the repository, you will need to create a personal access token, save it somewhere secure, and use it in lieu of your account password when prompted. For further instructions, see <ulink url="https://software.nersc.gov/help/user/profile/personal_access_tokens"/>. This page can only be accessed once you have logged in to software.nersc.gov by following the first link. The template .bashrc file from this repository can be copied into your home directory. Alternatively you can create your own dotfiles.
      </para>
      <para>
	Your desired environment configuration may differ depending on whether or not you are inside a Shifter container. The following if-else block (or similar) can be included in your .bashrc file to conditionally setup your environment
	<informalexample>
	  <programlisting>
	    if [ -z "$SHIFTER_RUNTIME" ]; then
	        : # Settings for when *not* in shifter
	    else
	        : # Settings for when *in* shifter
	    fi
	  </programlisting>
	</informalexample>
	as the environment variable <literal>SHIFTER_RUNTIME</literal> is equal to 1 if you are inside a Shifter container and a null string otherwise. An example .bashrc file derived from the NERSC dotfiles GitLab repository is provided in <xref linkend="app_bashrc"/>. 
      </para>
    </section>

    <section id="sec_filesystems">
      <title>NERSC Filesystems and Transferring Data</title>
      <para>
	NERSC has a variety of file systems with different levels of performance, data persistence and capacity which are configured for different purposes. See the NERSC file system documentation for details: <ulink url="https://docs.nersc.gov/filesystems"/>. Data is shared within a project using the Community File System (CFS) which is set with group read/write permissions by default. The CFS space for this project is located at /global/cfs/cdirs/m4386. The environment variable <literal>CFS</literal> is automatically set by NERSC and expands to /global/cfs/cdirs. Project CFS space is intended for data storage, not for intensive I/O jobs.
      </para>
      <para>
	User accounts also have space on the Perlmutter scratch file system, which can be referenced using the environment variable <literal>PSCRATCH</literal> or <literal>SCRATCH</literal> which expands to /pscratch/sd/&lt;first_letter_of_username&gt;/&lt;username&gt; (<emphasis>ASC note: e.g. my account scratch space is /pscratch/sd/c/chester</emphasis>). Shifter can create and access local, temporary xfs files on the Lustre file system for I/O intensive jobs. Job output can be staged on the xfs space but must be moved to permanent storage (most likely, to the project CFS space) at the end of a job. Temporary file space can be requested by adding the following line to a batch script: <literal>#SBATCH --volume="/global/cscratch1/sd/&lt;username&gt;/tmpfiles:/tmp:perNodeCache=size=200G"</literal> which will create a 200 GB xfs file mounted at /tmp in the image. You must replace <literal>&lt;username&gt;</literal> with your own user name. 
      </para>
      <para>
	Transferring small files into and out of NERSC i.e. for testing, can be done using SCP and a NERSC data transfer node (DTN). Larger file transfers i.e. for experimental data sets should be done with Globus. The SDAQ group is responsible for managing Globus transfers. For small transfers, data can be sent to NERSC over a DTN using the command <command>scp /local/path/to/file &lt;username&gt;@dtn01.nersc.gov:/remote/path/to/file</command> and retrieved from NERSC using the command <command>scp &lt;username&gt;@dtn01.nersc.gov:/remote/path/to/file /local/path/to/file</command> where again <literal>&lt;username&gt;</literal> must be replaced with your user account name; see <ulink url="https://docs.nersc.gov/services/scp"/>. For example, to transfer a file from a fishtank machine at FRIB to my (<emphasis>ASC</emphasis>) NERSC home directory, I ran the command <command>scp helloworld_mpi.tar.gz chester@dtn01.nersc.gov:/global/homes/c/chester</command>. There are four DTNs at NERSC you can use to transfer files: dtn0[1234].nersc.gov. Transferring files will require you to provide both your password and an OTP.
      </para>
    </section>
  </chapter>
  
  <chapter id="ch_plugins">
    <title>Fit Plugin Libraries for DDASToys</title>    
    <para>
      This chapter describes the fitting plugins for the <literal>EventEditor</literal> framework in broad terms. The required components are listed below, with the template fitting method given as an example. Fit plugin libraries used with the <literal>EventEditor</literal> must contain the following:
    </para>
    <itemizedlist>
      <listitem>
	<para>
	  An editor class - A method for extending hits with fit information called when the <command>EventEditor</command> command is executed. The editor method subclasses <literal>CBuiltRingItemEditor::BodyEditor</literal> to modify event fragment body data and implements its mandatory interface to edit the event body and free dynamic extension data. The editor is responsible for determining which channels should be fit, getting trace data from the event fragment and performing fits by calling the appropriate fitting subroutines. The data structures containing the fit information appended to each hit are defined in the fit_extensions.h header file. See FitEditorTemplate.h/cpp and its Doxygen documentation for more details. The editor class must contain a factory method called <literal>createEditor()</literal> to create itself when the <literal>EventEditor</literal> executable is run:	  
	  <informalexample>
	    <programlisting>
	      extern "C" {
	          FitEditorTemplate* createEditor() {
	              return new FitEditorTemplate;
	          }
	      }
	    </programlisting>
	  </informalexample>
	  Wrapping the factory method in <literal>extern "C"</literal> is necessary to prevent name mangling by the C++ compiler.	    
	</para>
      </listitem>
      <listitem>
	<para>
	  A fitting method - Generally speaking, a fitting method is composed of three parts:
	  <orderedlist>
	    <listitem>
	      <para>
		A model which describes the data to be fit,
	      </para>
	    </listitem>
	    <listitem>
	      <para>
		An objective function relating the model and data and
	      </para>
	    </listitem>
	    <listitem>
	      <para>
		An algorithm to optimize the objective function.
	      </para>
	    </listitem>
	  </orderedlist>
	  For template trace fitting, the model is a template trace parameterized by an amplitude, position and constant baseline offset, the objective function is Neyman's chi-square (both implemented in functions_template.h/cpp) and the optimization algorithm, implemented in lmfit_template.h/cpp, uses the LM method in GSL to minimize the chi-square value between the model and trace data. See the Doxygen documentation for more detail on the fitting subroutines.
	</para>
      </listitem>
      <listitem>
	<para>
	  A library plugin - A shared library containing the editor and fitting subroutines described above which are loaded at runtime when executing the <command>EventEditor</command> command. The plugin library <literal>libFitEditorTemplate.so</literal> provides the interface to the template fitting method listed above.
	</para>
      </listitem>
    </itemizedlist>
    <para>
      Additionally, it is helpful to supply the following:
    </para>    
    <itemizedlist>
      <listitem>
	<para>
	  An unpacker library - An optional library implementing methods for unpacking event fragments with fit extensions into a useful data structure for downstream analysis. The <literal>libDDASFitHitUnpacker.so</literal> library provides an interface to unpack DDAS events with fit extensions into the data structure defined by the <literal>DDASFitHit</literal> class. <literal>DDASFitHit</literal> is derived from the <literal>DDASHit</literal> class, which is defined in the header file of the same name found in the FRIBDAQ include directory, pointed to by the <literal>DAQINC</literal> environment variable provided that the FRIBDAQ environment has been configured by sourcing its <literal>daqsetup.sh</literal> script. Again, refer to the Doxygen documentation for more information.
	</para>
      </listitem>
    </itemizedlist>
  </chapter>
  
  <chapter id="ch_usingddastoys">
    <title>Data Analysis with DDASToys</title>
    <para>
      If you have read to this point, you are probably interested in processing some data at NERSC. This chapter is for you: a how-to guide for analyzing experimental data using the DDASToys tools. Included in this chapter are example job submission scripts which can be run under Slurm at NERSC. Jobs are submitted to Slurm using the <command>sbatch</command> command with the syntax <literal>sbatch [options] script [args_passed_to_script]</literal>; available options can be viewed by running the command <command>sbatch -h</command> from the terminal on a NERSC login node. The DDASToys tools and job submission scripts must be run under the Shifter container described in <xref linkend="ch_shifter"/>. <xref linkend="app_troubleshooting"/> contains solutions to some commonly encountered issues running DDASToys code. 
    </para>

    <section id="sec_importantinfo">
      <title>Project Information</title>
      <para>
	Information about the project, data storage, and software is summarized here:
      </para>
      <itemizedlist>
	<listitem>
	  <para>
	    <emphasis role="strong">Project name:</emphasis> m4386
	  </para>
	</listitem>
	<listitem>
	  <para>
	    <emphasis role="strong">Project storage:</emphasis> /global/cfs/cdirs/m4386
	  </para>
	</listitem>
	<listitem>
	  <para>
	    <emphasis role="strong">usropt tree:</emphasis> /global/cfs/cdirs/m4386/opt-buster.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    <emphasis role="strong">Experimental data:</emphasis> /global/cfs/cdirs/m4386/experiment (and subdirectories).
	  </para>
	</listitem>
      </itemizedlist>     
    </section>
    
    <section id="sec_queuesandqos">
      <title>Job Queues and QOS</title>
      <para>
	Running jobs on NERSC systems will charge hours to the allocation assigned to the project specified by the <literal>--account</literal> flag when the <command>sbatch</command> is invoked. The project for FDSi e21062 analysis is m4386. Jobs at NERSC can be submitted via a number of different queues which each provide a level of service depending on the user's needs and the constraints of the queue itself. The charge factor of the job, and therefore the calculated hours charged to the project allocation depend on which queue the job was run under. Please read <ulink url="https://docs.nersc.gov/jobs/policy/"/> for more information about the job queues available at NERSC and their impact on the number of charged hours . All examples given in this document use the "shared" queue which allows jobs from other users to run simultaneously on the same node. To run jobs exclusively on each node, use the "regular" queue.
      </para>
    </section>
  
    <section id="sec_configfiles">
      <title>Configuration Files Required by the <literal>EventEditor</literal></title>
      <para>
	Here we return to our discussion from <xref linkend="ch_shifter"/> of the environment variable file provided as an argument to the <command>shifter</command>. It is assumed in this section that the environment variable file is called shifter.env and is located in your user account's home directory. The shifter.env file contains a list of environment variables to set inside the container when it is passed as an argument to the <command>shifter</command> command using <literal>--env-file=$HOME/shifter.env</literal>:
      </para>
      <informalexample>
	<programlisting>
	  OPAL_PREFIX=/usr/opt/mpi/openmpi-4.0.1                   <co id="ompi"/>
	  FIT_CONFIGFILE=/global/homes/c/chester/fitconfig.txt     <co id="fitconfig"/>
	  TEMPLATE_CONFIGFILE=/global/homes/c/chester/template.txt <co id="templateconfig"/>
	  ROOT_INCLUDE_PATH=/usr/opt/ddastoys/include              <co id="rootinc"/>
	</programlisting>
      </informalexample>
      <calloutlist>
	<callout arearefs="ompi">
	  <para>
	    Path to the top-level installation directory of OpenMPI inside the container as described in <xref linkend="ch_shifter"/>. The Open Portable Access Layer (OPAL) is one of the core modules of OpenMPI.
	  </para>
	</callout>
	<callout arearefs="fitconfig">
	  <para>
	    Defines the <literal>FIT_CONFIGFILE</literal> environment variable and location of the fit configuration file needed by the <literal>EventEditor</literal> program.
	  </para>
	</callout>
	<callout arearefs="templateconfig">
	  <para>
	    Defines the <literal>TEMPLATE_CONFIGFILE</literal> environment variable and location of the template configuration file needed by the <literal>EventEditor</literal> program.
	  </para>
	</callout>
	<callout arearefs="rootinc">
	  <para>
	    Include path for DDASToys headers needed by the ROOT dictionary that defines the output data structure.
	  </para>
	</callout>
      </calloutlist>
      <para>
	The structure of the configuration files pointed to by the <literal>FIT_CONFIGFILE</literal> and <literal>TEMPLATE_CONFIGFILE</literal> environment variables are described below. The fitting routines expect the configuration files to have the formats defined in this section and will fail to run correctly if an improperly formatted file is provided. 
      </para>
      <para>
	The fit configuration file pointed to by the <literal>FIT_CONFIGFILE</literal> environment variable is a multi-line, whitespace-separated text file describing which channels should be fit. The file can contain blank lines. Both leading and trailing whitespaces are ignored. If the first non-whitespace character on a line is `#', the line is considered a comment and ignored. Non-commented lines must contain six whitespace-separated unsigned integers and one string:
      </para>
      <itemizedlist>
	<listitem>
	  <para>
	    crate - The crate ID for the channel to be fit.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    slot - The slot ID for the channel to be fit. The first slot with an installed module is typically slot 2.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    channel - The channel ID on the above crate/slot which will be fit.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    first-point - Sample number on the recorded trace defining the inclusive low limit of the fitting range. This parameter is ignored by the machine-learning inference fitting, but will be read by traceview to plot the fit data, so it is best practice to set to some reasonable value.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    last-point - Sample number on the trace defining the inclusive high limit of the fitting range. This parameter is ignored by the machine-learning inference fitting, but will be read by traceview to plot the fit data, so it is best practice to set to some reasonable value.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    saturation - The saturation value for the ADC in this crate/slot. Points on the trace greater than or equal to the saturation value are not included in the fit. This allows for fitting traces which overflow the ADC. For a module with a bit depth of B bits, the maximum ADC value is given by 2<superscript>B</superscript>-1.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    model - Path to a PyTorch model for ML inference. This parameter is ignored for fitting which does not implement the machine learning. An empty string is considered a vaild input.
	  </para>
	</listitem>	
      </itemizedlist>
      <para>
	Shown below is an example fit configuration file to fit a single channel, showing the use of comments which can provide more detailed identifying information for the channels to be fit, in this case the dynode of an implantation detector used in a beta-decay experiment. Note that in the model path is "", indicating that this configuration file was not written for ML inference.
      </para>
      <informalexample>
	<programlisting>
	  #
	  #   Describes which channels should be fit.
	  #   crate slot channel first-point last-point saturation model
	  #

	  #  Gamma crate high-gain YSO dynode:

	  1 2 0 1 128 65535 ""
	</programlisting>
      </informalexample>
      <para>
	The template configuration file pointed to by the <literal>TEMPLATE_CONFIGFILE</literal> environment variable is also a multi-line, whitespace-separated text file, this time containing template metadata and the template trace values. The rules regarding whitespace, blank lines, and comments are the same as for the fit configuration file. The first non-comment line contains two whitespace-separated unsigned integers which define the template metadata:
      </para>
      <itemizedlist>
	<listitem>
	  <para>
	    align-point - Time reference point for the template in samples. The position of the fitted pulse (in samples) reported by the template trace fit is given with respect to the align-point.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    npts - Number of data points in the template trace. 
	  </para>
	</listitem>	
      </itemizedlist>
      <para>
	The remaining lines contain the floating-point template data values. Shown below is an abbreviated example template configuration file.
      </para>
      <informalexample>
	<programlisting>
	  #
	  # Template metadata and values.
	  #
	  #   align-point npts
	  #   template[0]
	  #   template[1]
	  #   template[2]
	  #   ...
	  #   template[npts-1]
	  #
	  54 130
	  -2.8664e-06
	  -3.03525e-06
	  1.13924e-06
	  ...
	  -0.0457651
	</programlisting>
      </informalexample>
    </section>

    <section id="sec_configlegacy">
      <title>Configuration Files for DDASToys Major Versions 4 and 5</title>
      <para>
	Legacy fit configuration files for DDASToys major versions 4 and 5 must omit the model path string. An example config file is provided below.
      </para>
      <informalexample>
	<programlisting>
	  #
	  #   Describes which channels should be fit.
	  #   crate slot channel first-point last-point saturation
	  #

	  #  Gamma crate high-gain YSO dynode:

	  1 2 0 1 128 65535
	</programlisting>
      </informalexample>
    </section>
    
    <section id="sec_singlejob">
      <title>Running a Single <literal>EventEditor</literal> Job</title>
      <para>
	The fribdaq/frib-buster:v4.2 container supports FRIBDAQ 12.0 and later which include parallel event editors. The FRIBDAQ <literal>EventEditor</literal> program provides the architecture for fitting event fragments containing traces. Plugin libraries which define the fitting algorithms and the data structure containing the fit results appended to each hit are provided by the user and loaded at the <literal>EventEditor</literal> program runtime. The <literal>EventEditor</literal> program supports parallel fitting using both ZMQ threading and OpenMPI. Running the command <command>$DAQBIN/EventEditor --help</command> from inside the container describes the flags which can be passed to the program assuming the FRIBDAQ environment has been configured by sourcing its associated daqsetup.bash script.
      </para>
      <para>
	Shown below is an example Slurm job submission script which will configure the proper runtime environment and run the <literal>EventEditor</literal> program to fit traces using the template fitter. More information on how to run jobs at NERSC and how to use the Slurm batch system can be found at <ulink url="https://docs.nersc.gov/jobs/"/>. To submit a job using the following submission script, run the command <command>sbatch submit.sl</command>.
      </para>
      <informalexample>
	<programlisting>
	  #!/bin/bash

	  ##
	  # submit.sl
	  #
	  # Usage: sbatch submit.sl
	  #
	  # This file submits the fitter run inside the container. --ntasks are
	  # MPI workers + 3. 30 mins run time is long enough for single
	  # segments I think.
	  # 

	  #SBATCH --account=m4386                                            <co id="sbatchflags"/>
	  #SBATCH --licenses=scratch,cfs
	  #SBATCH --constraint=cpu
	  #SBATCH --qos=shared
	  #SBATCH --ntasks=64
	  #SBATCH --nodes=1
	  #SBATCH --time=00:30:00
	  #SBATCH --output=slurm_output/slurm-fitoutput-%j.out

	  #SBATCH --image=fribdaq/frib-buster:v4.2                           <co id="shifterflags"/>
	  #SBATCH --volume="/global/cfs/cdirs/m4386/opt-buster:/usr/opt;     \
	  /global/cfs/cdirs/m4386/experiment:/rawdata;                       \
	  /global/cfs/cdirs/m4386/chester/fitted:/fitted;                    \
	  /global/cscratch1/sd/chester/tmpfiles:/tmp:perNodeCache=size=100G"
	  #SBATCH --module=none

	  shifter --env-file=$HOME/shifter.env $HOME/run_fit.sh              <co id="shifterrun"/>
	</programlisting>
      </informalexample>
      <calloutlist>
	<callout arearefs="sbatchflags">
	  <para>
	    Flags specifying job information such as the project account to charge, number of tasks, and maximum allowed wall time. These flags are passed to <literal>sbatch</literal> when it is invoked to submit a job. The <literal>--constraint=cpu</literal> flag specifies that this job will be run on Perlmutter CPU nodes rather than GPU nodes. Output from this job will be written to the file specified by the <literal>--output</literal> flag, in this case slurm_output/slurm-fitoutput-%j.out where %j is the Slurm job ID.
	  </para>
	</callout>
	<callout arearefs="shifterflags">
	  <para>
	    Flags for running the job in our container. The image, volume mounting points and modules can be provided via the <literal>SBATCH</literal> directive but the environment file must be specified when invoking <literal>shifter</literal>. The volume mount points are shown here on multiple lines for clarity but must be provided on a single, semicolon-delimited line in a real submission script.
	  </para>
	</callout>
	<callout arearefs="shifterrun">
	  <para>
	    Use the <command>shifter</command> command to run the script <literal>$HOME/run_fit.sh</literal> inside our container. Environment variables defined in the file <literal>$HOME</literal>/shifter.env will be set inside the container.
	  </para>
	</callout>
      </calloutlist>
      <para>
	The script <literal>run_fit.sh</literal> calls the <literal>EventEditor</literal> to fit trace data in an FRIBDAQ event file:
      </para>
      <informalexample>
	<programlisting>
	  #!/bin/bash

	  ##
	  # run_fit.sh
	  #
	  # Usage: Run under a shifter container as part of an Slurm job.
	  #
	  # Run the EventEditor to fit traces in an .evt file. Stage I/O in
	  # /tmp space, move tmp output to CFS on completion. Time fitting
	  # call and write to Slurm output file
	  #
	  
	  cd $HOME

	  . /usr/opt/daq/12.0-005/daqsetup.bash -f            <co id="rte"/>

	  export PATH=$OPAL_PREFIX/bin:$PATH
	  export LD_LIBRARY_PATH=$OPAL_PREFIX/lib:$LD_LIBRARY_PATH

	  input=/rawdata/run283/run-0283-00.evt               <co id="stagefiles"/>
	  output=/fitted/$(echo $input | cut -d '/' -f 4 | cut -d '.' -f 1)-fitted.evt
	  tmpin=/tmp/tmpin-$SLURM_JOB_ID.evt
	  tmpout=/tmp/tmpout-$SLURM_JOB_ID.evt
	  cp $input $tmpin
	  
	  nworkers=`expr $SLURM_NTASKS - 3`                   <co id="setworkers"/>

	  echo "JobID   $SLURM_JOB_ID"                        <co id="write_to_outfile"/>
	  echo "Time    $SLURM_JOB_START_TIME"
	  echo "Node    $SLURMD_NODENAME"
	  echo "Tasks   $SLURM_NTASKS"
	  echo "Workers $nworkers"
	  echo "FileIn  $input"
	  echo "FileOut $output"
	  echo
	  
	  time mpirun -np $SLURM_NTASKS $DAQBIN/EventEditor \ <co id="exec"/>
	  -s file://$tmpin                                  \ <co id="source"/>
	  -S file://$tmpout                                 \ <co id="sink"/>
	  -l /usr/opt/ddastoys/lib/libFitEditorTemplate.so  \ <co id="lib"/>
	  -n $nworkers                                      \ <co id="mpiworkers"/>
	  -c 2000                                           \ <co id="clump"/>
	  -p mpi                                              <co id="strat"/>

	  rm $tmpin                                           <co id="cleanup"/>
	  mv $tmpout $output
	</programlisting>
      </informalexample>      
      <calloutlist>
	<callout arearefs="rte">
	  <para>
	    Configure the runtime environment inside the container. Since the program we are running depends on FRIBDAQ, ensure that the FRIBDAQ environment is set up. The <literal>-f</literal> flag will overwrite any existing FRIBDAQ environment variables when the <literal>daqsetup.bash</literal> script is sourced. Additionally, we must add OpenMPI executables and libraries to <literal>PATH</literal> and <literal>LD_LIBRARY_PATH</literal> inside the container, respectively. The environment variable <literal>OPAL_PREFIX</literal> is assumed to point to the top-level installation directory of OpenMPI inside our container image.
	  </para>
	</callout>
	<callout arearefs="stagefiles">
	  <para>
	    The following five lines define the input and output files in the account CFS space (<literal>input</literal> and <literal>output</literal>), temporary (staged) input and output files for the job (<literal>tmpin</literal> and <literal>tmpout</literal>), and copy the file pointed to by <literal>input</literal> to the locally staged input file <literal>tmpin</literal>. Temporary output for the job is staged in using the xfs file mounted at /tmp in the image. The final output file is saved to /output/run-0283-00-fitted.evt. The output file name is automatically generated from the input file name.
	  </para>
	</callout>
	<callout arearefs="setworkers">
	  <para>
	    Determine the number of MPI processes to use for parallel fitting. The number of worker processes is given by <literal>$SLURM_NTASKS - 3</literal> because in addition to the fit tasks, the MPI fitter reserves 3 tasks for distributing data, sorting the fitted data and outputting the sorted data to disk. For MPI fitting, this means the number of Slurm tasks must be at least 4.
	  </para>
	</callout>
	<callout arearefs="write_to_outfile">
	  <para>
	    Use <literal>sbatch</literal> environment variables to write some job details to the job's output file. A full list of variables can be found at <ulink url="https://slurm.schedmd.com/sbatch.html"/> 
	  </para>
	</callout>
	<callout arearefs="exec">
	  <para>
	    Use <literal>mpirun</literal> to run the <literal>EventEditor</literal> with the number of processes <literal>-np</literal> specified by the number of Slurm tasks. Because we previously setup the FRIBDAQ environment, <literal>$DAQBIN</literal> points to the correct directory for FRIBDAQ binaries.
	  </para>
	</callout>
	<callout arearefs="source">
	  <para>
	    The <literal>-s</literal> or <literal>--source</literal> option specifies the input URI. In this case we read data from the file pointed to by <literal>tmpin</literal>, which is located in the temporary xfs space on the node where the job is running.
	  </para>
	</callout>
	<callout arearefs="sink">
	  <para>
	    The <literal>-S</literal> or <literal>--sink</literal> option specifies the output URI. In this case we write temporary output data to the xfs file created on this node.
	  </para>
	</callout>
	<callout arearefs="lib">
	  <para>
	    The <literal>-l</literal> or <literal>--classifier</literal> option specifies the location of the plugin library that describes the single- and double-pulse fits performed for each trace as well as the structure of the data appended to each event fragment. The extension added to each trace is described in include/fit_extensions.h under the top level DDASToys installation directory; refer to the documentation for the <literal>HitExtensions</literal> struct in the <literal>DDAS</literal> namespace for details. The sizes for the whole event and for the fragment containing fit data are updated to be consistent with the updated fragment size(s). 
	  </para>
	</callout>
	<callout arearefs="mpiworkers">
	  <para>
	    The <literal>-n</literal> or <literal>--workers</literal> option specifies the number of worker processes used to fit traces. For MPI fitting this is given by the number of Slurm tasks minus the three processes reserved for I/O and sorting.
	  </para>
	</callout>
	<callout arearefs="clump">
	  <para>
	    The <literal>-c</literal> or <literal>--clump-size</literal> option determines how many events get passed as a work unit to each worker thread. A work unit of 2000 events was found to consistently achieve the best data processing rate over a large range of MPI workers.
	  </para>
	</callout>
	<callout arearefs="strat">
	  <para>
	    The <literal>-p</literal> or <literal>--parallel-strategy</literal> option sets the parallel strategy enabling nearline fitting of trace data. Here the fitting is parallelized using MPI.
	  </para>
	</callout>
	<callout arearefs="cleanup">
	  <para>
	    Delete temporary input file(s) and move the temporary output to the project CFS storage space. This step is critical for jobs utilizing temporary xfs space for I/O. Data stored in the temporary file space will not persist beyond the end of the job.
	  </para>
	</callout>
      </calloutlist>
      <para>
	The <literal>EventEditor</literal> program also requires (at minimum) the environment variable <literal>FIT_CONFIGFILE</literal> to be defined and point to a configuration file which describes the channels to be fit. Additionally, the template fitting library plugin requires the environment variable <literal>TEMPLATE_CONFIGFILE</literal> to be defined and point to a file containing template metadata and the template trace used for fitting. The configuration files and their expected formats are discussed further in <xref linkend="sec_configfiles"/>.
      </para>
    </section>

    <section id="sec_jobarrays">
      <title>Job Arrays</title>
      <para>
	Job arrays allow a collection of similar jobs to be submitted and managed together in Slurm. All individual jobs must have the same initial options: the values set using the <literal>--time</literal>, <literal>--ntasks</literal>, etc. flags must be identical. The <literal>--array</literal> flag sets a range of index values which can be accessed by an individual task via the <literal>SLURM_ARRAY_TASK_ID</literal> environment variable; for further information about submitting batch job arrays see <ulink url="https://slurm.schedmd.com/job_array.html"/>. Individual array tasks are interpreted by Slurm as single jobs which may impact their scheduling priority. An annotated example submission script to fit traces in multiple subruns of a single run, indexed using a job array, is given in <xref linkend="app_jobarray"/>.
      </para>
    </section>
    
    <section id="sec_traceview">
      <title>Viewing Fit Results with <literal>traceview</literal></title>
      <para>
	The <literal>traceview</literal> program provides a graphical interface to view event data containing traces as well as their associated fits and classification as single- or double-pulse events if the machine learning pulse classifier is used. The viewer is installed in the /bin directory under the top-level DDASToys installation directory. Interfaces to display subsets of hit data and select the fitting method are provided. <literal>traceview</literal> reads fit and template configuration information from the <literal>FIT_CONFIGFILE</literal> and <literal>TEMPLATE_CONFIGFILE</literal> environment variables and will exit with an error message if these environment variables are undefined or point to improperly formatted files. The <literal>traceview</literal> display is a ROOT TCanvas and can be interacted with as such.
      </para>
    </section>

    <section id="sec_eeconverter">
      <title>Using <literal>eeconverter</literal> to Convert <literal>EventEditor</literal> Output to ROOT Format</title>
      <para>
	Similar to the <literal>ddasdumper</literal>, the DDASToys <literal>eeconverter</literal> program converts FRIBDAQ event file data containing traces and fit information into ROOT format for downstream analysis. A shared library for I/O in ROOT is provided. Running <command>/usr/opt/ddastoys/bin/eeconverter --help</command> from the command line inside the container will show how to use the program and how to pass it arguments; running without any command line parameters shows the minimum number of required arguments. A sample job submission script and the script it calls to run the <literal>eeconverter</literal> program are described in <xref linkend="app_eeconverter"/>.
      </para>
    </section>

    <section id="sec_pipeline">
      <title>Creating an Analysis Pipeline Using <literal>sbatch</literal> Job Dependencies</title>
      <para>
	Slurm job dependencies allow the user to delay the start of a job contingent on some set of conditions, including, for example, the successful completion of another job. Complex analysis pipelines can thus be constructed and submitted to the Slurm scheduler using a single script. As an example, consider an analysis workflow using the DDASToys codes described in the previous sections to fit trace data and convert all the fitted output to ROOT format:
	<orderedlist>
	  <listitem>
	    <para>
	      Using job arrays, and,
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      Using a single script with dependencies.
	    </para>
	  </listitem>
	</orderedlist>
	Sample scripts to perform this combined analysis and be found in <xref linkend="app_eeconverter"/>. More information on creating analysis pipelines can be found on both the NERSC and Slurm documentation pages: <ulink url="https://docs.nersc.gov/jobs/examples/#dependencies"/> and <ulink url="https://slurm.schedmd.com/sbatch.html"/>, respectively. 
      </para>
    </section>

    <section id="sec_scripts">
      <title>Example Scripts</title>
      <para>
	Example job submission scripts for the analyses discussed in this section are available at NERSC under /global/cfs/cdirs/m4386/example_scripts. To use these scripts, copy them to your home directory and modify as needed, by, for example, changing the paths of the volume mounts to point to your analysis directories (<emphasis>ASC: rather than mine, in /chester under the top-level project directory</emphasis>). If you create a new directory for your analysis under the top-level project directory at $CFS/m4386, remember to set the correct permissions if you intend to mount it in the image, see <xref linkend="sec_bindpoints"/>. A brief summary of the available examples are provided here:
      </para>
      <itemizedlist>
	<listitem>
	  <para>
	    <literal>submit_fit.sh</literal> -- Prepare a submission script and submit an <literal>EventEditor</literal> job via <literal>sbatch</literal>. A simplified version of this process is described in <xref linkend="sec_singlejob"/>
	  </para>
	</listitem>
	<listitem>
	  <para>
	    <literal>run_fit.sh</literal> - Run the <literal>EventEditor</literal> to fit traces in an event file. This script is called by <literal>submit_fit.sh</literal>. See <xref linkend="sec_singlejob"/> for details.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    <literal>submit_fitarr.sh</literal> - Prepare a submission script and submit an <literal>EventEditor</literal> job array via <literal>sbatch</literal>. Discussed further in <xref linkend="sec_jobarrays"/> with annotated examples given in <xref linkend="app_jobarray"/>.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    <literal>run_fitarr.sh</literal> - Run the <literal>EventEditor</literal> to fit traces in an event file as an array task. This script is called by<literal>submit_fitarr.sh</literal>. See <xref linkend="sec_jobarrays"/> and <xref linkend="app_jobarray"/> for details.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    <literal>submit_converter.sh</literal> - Prepare a submission script and submit an <literal>eeconverter</literal> job to convert an event file to ROOT format. Additional information is provided in <xref linkend="sec_eeconverter"/> and <xref linkend="app_eeconverter"/>.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    <literal>run_converter.sh</literal> - Run an <literal>eeconverter</literal> task to convert <literal>EventEditor</literal> output to ROOT format. See <xref linkend="sec_eeconverter"/> for more information and <xref linkend="app_eeconverter"/> for a sample script.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    <literal>submit_combined.sh</literal> - Submit array jobs for fitting traces and conversion to ROOT format using a single script, where the conversion to ROOT format depends on successful completion of the fitting job. This script uses the <literal>--parsable</literal> and <literal>--dependency</literal> flags to control the analysis workflow. Creating a combined analysis pipeline is discussed in <xref linkend="sec_pipeline"/>. See <xref linkend="app_pipeline"/> for sample scripts.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    <literal>run_converterarr.sh</literal> - Run an <literal>eeconverter</literal> array task to convert <literal>EventEditor</literal> output to ROOT format as part of the combined analysis pipeline described in <xref linkend="sec_pipeline"/>. A sample script is provided in <xref linkend="app_pipeline"/>.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    <literal>shiftercmd</literal> - Run all passed arguments under the Shifter container described in <xref linkend="ch_shifter"/>. For example, <command>./shiftercmd /bin/bash</command> will spawn a Bash shell in the container. This script expects that environment variables you wish to propagate into the container are defined in a file called shifter.env located in your account home directory. The formatting and contents of this file are discussed in <xref linkend="sec_configfiles"/>. A sample shifter.env file is provided in the example directory.
	  </para>
	</listitem>
      </itemizedlist>
    </section>
    
  </chapter>

<appendix id="app_troubleshooting">
    <title>Troubleshooting DDASToys Code</title>
    <para>
      This appendix provides solutions for issues encountered while running DDASToys code.
    </para>
        <itemizedlist>
	  <listitem>
	    <para>
	      <emphasis>Issue:</emphasis> The <literal>EventEditor</literal> code crashes with no output and the following error message:
	      <informalexample>
		<programlisting>
		  [pike:48184] *** Process received signal ***
		  [pike:48184] Signal: Segmentation fault (11)
		  [pike:48184] Signal code: Address not mapped (1)
		  [pike:48184] Failing at address: 0x7fabb7027008
		  [pike:48184] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x12730)[0x7fabb80e5730]
		  [pike:48184] [ 1] /usr/opt/mpi/openmpi-4.0.1/lib/pmix/mca_gds_ds21.so \
		                    (pmix_gds_ds21_lock_init+0x129)[0x7fabb6e438d9]
		  ... more errors ...
		</programlisting>
	      </informalexample>
	      <emphasis>Solution:</emphasis> The last line shown above points to the issue, and the solution: disable the PMIx ds21 shared memory optimization by setting the environment variable <literal>PMIX_MCA_gds=^ds21</literal>. Note that this could be done in the shifter.env file used to define the runtime environment of DDASToys on a cluster such as NERSC.
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      <emphasis>Issue:</emphasis> Running the <literal>EventEditor</literal> in threaded mode results in no output with the following error:
	      <informalexample>
		<programlisting>
		  Warning ZMC Communicator factory created with empy endpoints table
		  terminate called after throwing an instance of 'std::invalid_argument'
		      what():  Failed to find endpoint: 1 in ZMQ endpoint lookup table
		  Aborted
		</programlisting>
	      </informalexample>
	      <emphasis>Solution:</emphasis> ZMQ is used for threaded parallelism. When using ZMQ transport, the program needs an endpoint lookup table to make a translation between numbered communication endpoints and the URI's that ZMQ uses. This is a text file that contains lines that consist of an endpoint number and a URI indicating the ZMQ communication URI to use. The file can be in ~/.zmqservices, `pwd`/.zmqservices or where the environment variable ZMQ_SERVICES points. The services that need to be specified are 1 - the fanout service, 2 - the sort service, and 3 - the output service. Here's the contents of an example ZMQ services file that I've used:
	    </para>
	    <informalexample>
		<programlisting>
		  1 inproc://distro
		  2 inproc://sort
		  3 inproc://sorted
		</programlisting>
	      </informalexample>
	  </listitem>
	  <listitem>
	    <literallayout>
	      <emphasis>Issue:</emphasis> The `traceview` canvas is empty despite selecting a trace to view.
	    <emphasis>Solution:</emphasis> The `traceview` canvas is an embedded ROOT canvas in a Qt application. You need to configure the ROOT environment by sourcing the appropriate thisroot.sh script.
	    </literallayout>
	  </listitem>
	</itemizedlist>    
  </appendix>
  
  <appendix id="app_bashrc">
    <title>Sample .bashrc File</title>
    <para>
      Important note: line breaks using the `\' character are shown for convenience and should be removed in a real .bashrc file.
    </para>
    <informalexample>
      <programlisting>
	# NERSC Defined Environment Variables
	# CFS - /global/cfs/cdirs
	# SCRATCH - /pscratch/sd/&lt;letter&gt;/$USER (Perlmutter)

	# System Environment Variables 

	# EDITOR set your preferred editor (vim, vi, emacs, nano)
	export EDITOR=/global/common/software/nersc/bin/emacs

	# configure user prompt see https://www.gnu.org/software/bash/manual   \
	/html_node/Controlling-the-Prompt.html
	# The prompt will be - username@hostname>
	# export PS1="\u@\h> "

	# useful alias
	alias perlmutter='ssh perlmutter.nersc.gov'
	alias squ="squeue -O 'UserName,State,Name,Partition,NumTasks,NumNodes, \
	BatchHost,TimeUsed,TimeLimit,SubmitTime,StartTime' -u $USER"
	alias sq="squeue -O 'UserName,State,Name,Partition,NumTasks,NumNodes,  \
	BatchHost,TimeUsed,TimeLimit,SubmitTime,StartTime'"

	# history setting https://www.gnu.org/software/bash/manual/html_node/  \
	Bash-History-Builtins.html
	export HISTFILESIZE=1000
	export HISTSIZE=1000
	export HISTTIMEFORMAT="%h %d %H:%M:%S "

	# user bin directory
	export PATH=$HOME/.local/bin:$PATH

	# terminal colors and safer file management
	alias ls='ls --color'
	alias l='ls --color'
	alias ll='ls --color -lh'
	alias lll='ls --color -alh'
	alias sl='ls --color | more'

	alias cp='cp -i'
	alias mv='mv -i'
	alias rm='rm -i'

      </programlisting>
    </informalexample>
  </appendix>
  
  <appendix id="app_jobarray">
    <title>Sample Scripts for Submitting <literal>EventEditor</literal> Job Arrays</title>
    <para>
      Shown below is a slightly more advanced job submission script which illustrates two concepts. First, the example demonstrates how to group similar jobs into a job array and submit the array as a batch job to Slurm. Second, this example shows how to prepare your user environment on a login node and submit a batch job in a single step as described in <ulink url="https://docs.nersc.gov/jobs/best-practices/"/>. This script should be run from the command line as <command>./submit_fitarr.sh</command>.
    </para>
    <informalexample>
      <programlisting>
	#!/bin/bash -l                                                     <co id="onlogin"/>

	##
	# Prepare a submission script and submit an EventEditor job array
	# via sbatch.
	# 

	# Prepare the user environment needed for the job. In this case, we
	# count the number of files in the input directory and use that count
	# to define the size of the job array:

	PROJECT=/global/cfs/cdirs/m4386
	FC=$(find $PROJECT/experiment/run283 -name "*.evt" | wc -l)
	ENDSEG=`expr $FC - 1`
	
	cat &lt;&lt;EOF &gt; fitarr_env.sl                                          <co id="catenv"/>
	#!/bin/bash

	##
	# This file was created automatically by submit_fitarr.sh and submits
	# fitting jobs in the container as a job array. --ntasks are MPI
	# workers + 3. 30 mins run time is long enough for single segments I
	# think. One array task per run segment.
	#
	
	#SBATCH --account=m4386
	#SBATCH --licenses=scratch,cfs
	#SBATCH --constraint=cpu
	#SBATCH --qos=shared
	#SBATCH --time=00:30:00
	#SBATCH --array=0-$ENDSEG                                          <co id="arrayflag"/>
	#SBATCH --ntasks=64                                                <co id="arraytasks"/>
	#SBATCH --nodes=1
	#SBATCH --output=slurm_output/slurm-fitoutput-%A_%a.out            <co id="outputflag"/>

	#SBATCH --image=fribdaq/frib-buster:v4.2
	#SBATCH --volume="$PROJECT/opt-buster:/usr/opt;     \
	$PROJECT/experiment:/rawdata;                       \
	$PROJECT/chester/fitted:/fitted;                    \
	/global/cscratch1/sd/chester/tmpfiles:/tmp:perNodeCache=size=100G"
	#SBATCH --module=none

	shifter --env-file=$HOME/shifter.env $HOME/runee_array.sh
	EOF

	sbatch fitarr_env.sl                                               <co id="sbatcharray"/>
      </programlisting>
    </informalexample>
    <calloutlist>
      <callout arearefs="onlogin">
	<para>
	  Run the script on a login node using <literal>-l</literal>.
	</para>
      </callout>
      <callout arearefs="catenv">
	<para>
	  Set up the environment for the batch job on the login node. Everything between this line and the next <literal>EOF</literal> will be written to a Slurm batch submission script called fitarr_env.sl.
	</para>
      </callout>
      <callout arearefs="arrayflag">
	<para>
	  The <literal>SBATCH</literal> directive to submit a job array. The number of array tasks is equal to the number of input files. For example, if the run directory contains 5 files, the directive would be <literal>#SBATCH --array=0-4</literal>.
	</para>
      </callout>
      <callout arearefs="arraytasks">
	<para>
	  Each task in the job array is considered a standalone job by the Slurm scheduler. Each will be allocated <literal>--ntasks</literal> tasks on <literal>--nodes</literal> nodes. <emphasis>(ASC note) Apologies for the use of "task" here to describe both a job corresponding to a particular array value and the resource request for that job. I am attempting to remain consistent with the nomenclature Slurm has adopted for itself.</emphasis>
	</para>
      </callout>
      <callout arearefs="outputflag">
	<para>
	  The patterns %A and %a are replaced in the output file by the job ID and array index ID, respectively. Analogous to the %j pattern used for non-array jobs.
	</para>
      </callout>
      <callout arearefs="sbatcharray">
	<para>
	  Use the <command>sbatch</command> command to submit the job script.
	</para>
      </callout>
    </calloutlist>
    <para>
      The script calling the fitter for the job array is shown below, with the array index used to parameterize the run segment to fit.
    </para>
    <informalexample>
      <programlisting>
	#!/bin/bash

	##
	# run_fitarr.sh
	#
	# Set up the runtime environment, stage I/O and run the EventEditor.
	# Use the array index to parameterize the subruns. After fitting
	# cleanup temporary input and move output to CFS.
	#
	
	cd $HOME

	. /usr/opt/daq/12.0-005/daqsetup.bash

	export PATH=$OPAL_PREFIX/bin:$PATH
	export LD_LIBRARY_PATH=$OPAL_PREFIX/lib:$LD_LIBRARY_PATH

	if [ "$SLURM_ARRAY_TASK_ID" -lt 10 ]; then              <co id="getsegments"/>
	seg=$(printf "%02d" $SLURM_ARRAY_TASK_ID)
	else
	seg=$SLURM_ARRAY_TASK_ID
	fi

	input=/rawdata/run283/run-0283-$seg.evt                 <co id="stagearray"/>
	output=/fitted/$(echo $input | cut -d '/' -f 4 | cut -d '.' -f 1)-fitted.evt
	tmpin=/tmp/tmpin-$SLURM_ARRAY_JOB_ID-$SLURM_ARRAY_TASK_ID.evt
	tmpout=/tmp/tmpout-$SLURM_ARRAY_JOB_ID-$SLURM_ARRAY_TASK_ID.evt
	cp $input $tmpin

	nworkers=`expr $SLURM_NTASKS - 3`

	echo "JobID   $SLURM_ARRAY_JOB_ID-$SLURM_ARRAY_TASK_ID" <co id="arrayoutput"/>
	echo "Time    $SLURM_JOB_START_TIME"
	echo "Node    $SLURMD_NODENAME"
	echo "Tasks   $SLURM_NTASKS"
	echo "Workers $nworkers"
	echo "FileIn  $input"
	echo "FileOut $output"
	echo
	
	time mpirun -np $SLURM_NTASKS $DAQBIN/EventEditor \
	-s file://$tmpin                                  \
	-S file://$tmpout                                 \
	-l /usr/opt/ddastoys/lib/libFitEditorTemplate.so  \
	-n $nworkers                                      \
	-c 2000                                           \
	-p mpi

	rm $tmpin
	mv $tmpout $output
      </programlisting>
    </informalexample>
    <calloutlist>
      <callout arearefs="getsegments">
	<para>
	  FRIBDAQ event file segment numbers contain at least two digits (i.e. run-1234-00.evt rather than run-1234-0.evt for the first segment in a run). This <literal>if-else</literal> block handles these cases. For array indices less than 10, the variable <literal>seg</literal> contains its two-digit representation i.e. for <literal>SLURM_ARRAY_TASK_ID</literal> = 4, <literal>seg</literal> = 04.
	</para>
      </callout>
      <callout arearefs="stagearray">
	<para>
	  Stage temporary input and output data in the xfs file allocated for this job. The input name uses the <literal>seg</literal> variable to correctly specify a run segment file name in the CFS storage space. Note that the temporary filenames are uniqueified using the Slurm environment variables <literal>SLURM_ARRAY_JOB_ID</literal> and <literal>SLURM_ARRAY_TASK_ID</literal>.
	</para>
      </callout>
            <callout arearefs="arrayoutput">
	<para>
	  Use job array variables <literal>SLURM_ARRAY_JOB_ID</literal> and <literal>SLURM_ARRAY_TASK_ID</literal> to write the job information to the output file.
	</para>
      </callout>
    </calloutlist>
  </appendix>

  <appendix id="app_eeconverter">
    <title>Sample Scripts for Converting <literal>EventEditor</literal> Output to ROOT Format</title>
    <para>
      Shown below is a sample job submission script for running an <literal>eeconverter</literal> batch job. This script uses the method shown in <xref linkend="app_jobarray"/> to prepare the submission script on a login node before submitting the job using the <command>sbatch</command> command. 
    </para>
    <informalexample>
      <programlisting>
	#!/bin/bash -l

	##
	# Prepare a submission script and submit an EEConverter job.
	#

	PROJECT=/global/cfs/cdirs/m4386
	
	cat &lt;&lt; EOF &gt; converter_env.sl
	#!/bin/bash

	##
	# This file was created automatically by submit_converter.sh and submits
	# a run file for conversion to ROOT format.
	# 

	#SBATCH --account=m4386                                            <co id="nodeconfig"/>
	#SBATCH --licenses=scratch,cfs
	#SBATCH --constraint=cpu
	#SBATCH --qos=shared
	#SBATCH --nodes=1
	#SBATCH --ntasks=1
	#SBATCH --time=00:30:00
	#SBATCH --output=slurm_output/slurm-converter-%j.out

	#SBATCH --image=fribdaq/frib-buster:v4.2
	#SBATCH --volume="$PROJECT/opt-buster:/usr/opt;     \
	$PROJECT/chester/fitted:/fitted;                    \
	$PROJECT/chester/converted:/converted;              \
	/global/cscratch1/sd/chester/tmpfiles:/tmp:perNodeCache=size=100G"
	#SBATCH --module=none

	shifter --env-file=$HOME/shifter.env $HOME/run_converter.sh
	EOF

	sbatch converter_env.sl
      </programlisting>
    </informalexample>
    <calloutlist>
      <callout arearefs="nodeconfig">
	<para>
	  The <literal>SBATCH</literal> directives for <literal>eeconverter</literal> job submission. One thing to note here is that the <literal>eeconverter</literal> uses only a single core (really, a single <emphasis>thread</emphasis>). This process should always be run on the shared queue.
	</para>
      </callout>
    </calloutlist>
    <para>
      The <literal>run_converter.sh</literal> script which calls the <literal>eeconverter</literal> program is shown below:
    </para>
    <informalexample>
      <programlisting>
	#!/bin/bash

	##
	# Convert EventEditor output to ROOT format
	#

	cd $HOME

	. /usr/opt/root/root-6.24.06/bin/thisroot.sh  <co id="eeconfig"/>

	input=/fitted/run-0283-00-fitted.evt          <co id="stageeeout"/>
	output=/converted/$(echo $input | cut -d '/' -f 3 | cut -d '.' -f 1).root
	tmpin=/tmp/tmpin-$SLURM_JOB_ID.evt
	tmpout=/tmp/tmpout-$SLURM_JOB_ID.evt
	cp $input $tmpin

	echo "JobID   $SLURM_JOB_ID"
	echo "Time    $SLURM_JOB_START_TIME"
	echo "Node    $SLURMD_NODENAME"
	echo "Tasks   $SLURM_NTASKS"
	echo "FileIn  $input"
	echo "FileOut $output"
	echo

	time /usr/opt/ddastoys/bin/eeconverter \      <co id="runeeconverter"/>
	-s file://$tmpin                       \      <co id="eeconvertersource"/>
	-f $tmpout                                    <co id="eeconverterfileout"/>

	rm $tmpin                                     <co id="cleanupee"/>
	mv $tmpout $output
      </programlisting>
    </informalexample>
    <calloutlist>
      <callout arearefs="eeconfig">
	<para>
	  Configure the environment needed to run the <literal>eeconverter</literal> program. Because this program converts output to ROOT format, ensure that the correct version of ROOT is configured by sourcing its <literal>thisroot.sh</literal>.
	</para>
      </callout>
      <callout arearefs="stageeeout">
	<para>
	  Stage input and output files in the temporary space allocated on the node where the job is running. The output file name is automatically generated from the input.
	</para>
      </callout>
      <callout arearefs="runeeconverter">
	<para>
	  Run the <literal>eeconverter</literal> program installed in the DDASToys directory tree.
	</para>
      </callout>
      <callout arearefs="eeconvertersource">
	<para>
	  The <literal>-s</literal> or <literal>--source</literal> option specifies the input URI. In this case we read data from the file pointed to by <literal>tmpin</literal>, which is located in the temporary xfs space on the node where the job is running.
	</para>
      </callout>
      <callout arearefs="eeconverterfileout">
	<para>
	  The <literal>-f</literal> or <literal>--fileout</literal> option specifies the output ROOT file path. In this case we write to file pointed to by <literal>tmpout</literal>, which is located in the temporary xfs space on the node where the job is running.
	</para>
      </callout>
            <callout arearefs="cleanupee">
	<para>
	  Remove the temporary input file and move the temporary output to its final destination in the project CFS space.
	</para>
      </callout>
    </calloutlist>
  </appendix>

  <appendix id="app_pipeline">
    <title>Sample Scripts for Constructing an Analysis Pipeline Using <literal>sbatch</literal> Dependencies</title>
    <para>
      Shown below is a sample job submission script for running combined analysis. This script uses the <literal>sbatch</literal> flags <literal>--parsable</literal> and <literal>--dependency</literal> flags to extract the job ID value and use it to define a workflow where the submission of the ROOT conversion job is dependent on the successful completion of the fitting job. Much of the content of these scripts is similar to what is presented in <xref linkend="ch_usingddastoys"/>, <xref linkend="app_jobarray"/> and <xref linkend="app_eeconverter"/>; please refer to those sections for further details.
    </para>
    <informalexample>
      <programlisting>
	#!/bin/bash -l

	##
	# Prepare submission scripts and submit fitting and ROOT conversion job
	# arrays in a single step. The ROOT conversion job depends on good completion
	# of the fit. --ntasks are MPI workers + 3. 30 mins run time is long enough
	# for single segments I think. One array task per run segment.

	# Prepare the user environment needed for the job. In this case, we count
	# the number of files in the input directory and use that count to define
	# the size of the job array:
	#

	PROJECT=/global/cfs/cdirs/m4386
	FC=$(find $PROJECT/experiment/run283 -name "*.evt" | wc -l)
	ENDSEG=`expr $FC - 1`

	# Generate a submission script for fitting:

	cat &lt;&lt;EOF &gt; fitarr_env_combined.sl                                  <co id="combinedfit"/>
	#!/bin/bash

	##
	# This file was created automatically by submit_combined.sh and submits fitting 
	# jobs in the container as a job array. --ntasks are MPI workers + 3. 30 mins 
	# run time is long enough for single segments I think. One array task per run 
	# segment.
	#

	#SBATCH --account=m4386
	#SBATCH --licenses=scratch,cfs
	#SBATCH --constraint=cpu
	#SBATCH --qos=shared
	#SBATCH --ntasks=64
	#SBATCH --nodes=1
	#SBATCH --time=00:30:00
	#SBATCH --array=0-$ENDSEG
	#SBATCH --output=slurm_output/slurm-fitoutput-%A_%a.out

	#SBATCH --image=fribdaq/frib-buster:v4.2
	#SBATCH --volume="$PROJECT/opt-buster:/usr/opt;     \
	$PROJECT/experiment:/rawdata;                       \
	$PROJECT/chester/fitted:/fitted;                    \
	/global/cscratch1/sd/chester/tmpfiles:/tmp:perNodeCache=size=100G"
	#SBATCH --module=none

	shifter --env-file=$HOME/shifter.env $HOME/run_fitarr.sh           <co id="combinedcallfit"/>
	EOF

	# And similarly for the ROOT conversion:

	cat &lt;&lt;EOF &gt; converter_env_combined.sl                              <co id="combinedconvert"/>
	#!/bin/bash 

	##
	# This file was created automatically by submit_combined.sh and submits a run
	# file for conversion to ROOT format.
	# 

	#SBATCH --account=m4386
	#SBATCH --licenses=scratch,cfs
	#SBATCH --constraint=cpu
	#SBATCH --qos=shared
	#SBATCH --ntasks=1
	#SBATCH --nodes=1
	#SBATCH --time=00:30:00
	#SBATCH --array=0-$ENDSEG
	#SBATCH --output=slurm_output/slurm-converter-%A_%a.out

	#SBATCH --image=fribdaq/frib-buster:v4.2
	#SBATCH --volume="$PROJECT/opt-buster:/usr/opt;     \
	$PROJECT/chester/fitted:/fitted;                    \
	$PROJECT/chester/converted:/converted;              \
	/global/cscratch1/sd/chester/tmpfiles:/tmp:perNodeCache=size=100G"
	#SBATCH --module=none

	shifter --env-file=$HOME/shifter.env $HOME/run_converterarr.sh     <co id="combinedcallconvert"/>
	EOF

	# Here we actually submit the jobs. Conversion to ROOT conditional on the fit:

	jobid=$(sbatch --parsable fitarr_env_combined.sl)                  <co id="parsable"/>
	sbatch --dependency=afterok:$jobid converter_env_combined.sl       <co id="dependency"/>
      </programlisting>
    </informalexample>
        <calloutlist>
      <callout arearefs="combinedfit">
	<para>
	  Create a job submission script for the first step of the analysis pipeline. In this case we submit the fitter as a job array.
	</para>
      </callout>
      <callout arearefs="combinedcallfit">
	<para>
	  Run the <literal>EventEditor</literal> as an array task. The <literal>run_fitarr.sh</literal> script is described in <xref linkend="app_jobarray"/>.
	</para>
      </callout>
      <callout arearefs="combinedconvert">
	<para>
	  Create a job submission script for the second step of the analysis pipeline. In this case we submit the conversion as a job array.
	</para>
      </callout>
      <callout arearefs="combinedcallconvert">
	<para>
	  Run the <literal>eeconverter</literal> as an array task. This script is described in more detail later in this section.
	</para>
      </callout>
      <callout arearefs="parsable">
	<para>
	  Use the <literal>--parsable</literal> flag to store the Slurm job ID in the temporary variable <literal>jobid</literal>. This line submits the fitting job, and the ID assigned to this job by the Slurm scheduler is used to manage the workflow.
	</para>
      </callout>
      <callout arearefs="dependency">
	<para>
	  Use the <literal>--dependency</literal> flag to submit the converter job if and only if the fitting job specified by <literal>jobid</literal> has successfully executed (completed with return code 0).
	</para>
      </callout>
	</calloutlist>
	<para>
	  The script to run the <literal>eeconverter</literal> as a job array is shown below. There should be no surprises here:
	</para>
	<informalexample>
	  <programlisting>
	    #!/bin/bash

	    ##
	    # Set up the runtime environment, stage I/O and run the EEConverter to ROOT.
	    # Use the array index to parameterize the subruns. After conversion cleanup
	    # temporary input and move output to CFS
	    #

	    cd $HOME

	    . /usr/opt/root/root-6.24.06/bin/thisroot.sh

	    if [ "$SLURM_ARRAY_TASK_ID" -lt 10 ]; then
	    seg=$(printf "%02d" $SLURM_ARRAY_TASK_ID)
	    else
	    seg=$SLURM_ARRAY_TASK_ID
	    fi

	    input=/fitted/run-0283-$seg-fitted.evt
	    output=/converted/$(echo $input | cut -d '/' -f 3 | cut -d '.' -f 1).root
	    tmpin=/tmp/tmpin-$SLURM_ARRAY_JOB_ID-$SLURM_ARRAY_TASK_ID.evt
	    tmpout=/tmp/tmpout-$SLURM_ARRAY_JOB_ID-$SLURM_ARRAY_TASK_ID.evt
	    cp $input $tmpin

	    echo "JobID   $SLURM_ARRAY_JOB_ID_$SLURM_ARRAY_TASK_ID"
	    echo "Time    $SLURM_JOB_START_TIME"
	    echo "Node    $SLURMD_NODENAME"
	    echo "Tasks   $SLURM_NTASKS"
	    echo "FileIn  $input"
	    echo "FileOut $output"
	    echo

	    time /usr/opt/ddastoys/bin/eeconverter -s file://$tmpin -f $tmpout

	    rm $tmpin
	    mv $tmpout $output
	  </programlisting>
	</informalexample>
  </appendix>
  
</book>
